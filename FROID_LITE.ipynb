{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVbkG54Q9TEL"
      },
      "source": [
        "# IMPORT LIBRARIES AND FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sammon(x, n, display = 0, inputdist = 'raw', maxhalves = 20, maxiter = 1, tolfun = 1e-9, init = 'default'):\n",
        "\n",
        "    import numpy as np \n",
        "    from scipy.spatial.distance import cdist\n",
        "\n",
        "    \"\"\"Perform Sammon mapping on dataset x\n",
        "    y = sammon(x) applies the Sammon nonlinear mapping procedure on\n",
        "    multivariate data x, where each row represents a pattern and each column\n",
        "    represents a feature.  On completion, y contains the corresponding\n",
        "    co-ordinates of each point on the map.  By default, a two-dimensional\n",
        "    map is created.  Note if x contains any duplicated rows, SAMMON will\n",
        "    fail (ungracefully). \n",
        "    [y,E] = sammon(x) also returns the value of the cost function in E (i.e.\n",
        "    the ess of the mapping).\n",
        "    An N-dimensional output map is generated by y = sammon(x,n) .\n",
        "    A set of optimisation options can be specified using optional\n",
        "    arguments, y = sammon(x,n,[OPTS]):\n",
        "       maxiter        - maximum number of iterations\n",
        "       tolfun         - relative tolerance on objective function\n",
        "       maxhalves      - maximum number of step halvings\n",
        "       input          - {'raw','distance'} if set to 'distance', X is \n",
        "                        interpreted as a matrix of pairwise distances.\n",
        "       display        - 0 to 2. 0 least verbose, 2 max verbose.\n",
        "       init           - {'pca', 'cmdscale', random', 'default'}\n",
        "                        default is 'pca' if input is 'raw', \n",
        "                        'msdcale' if input is 'distance'\n",
        "    The default options are retrieved by calling sammon(x) with no\n",
        "    parameters.\n",
        "    File        : sammon.py\n",
        "    Date        : 18 April 2014\n",
        "    Authors     : Tom J. Pollard (tom.pollard.11@ucl.ac.uk)\n",
        "                : Ported from MATLAB implementation by \n",
        "                  Gavin C. Cawley and Nicola L. C. Talbot\n",
        "    Description : Simple python implementation of Sammon's non-linear\n",
        "                  mapping algorithm [1].\n",
        "    References  : [1] Sammon, John W. Jr., \"A Nonlinear Mapping for Data\n",
        "                  Structure Analysis\", IEEE Transactions on Computers,\n",
        "                  vol. C-18, no. 5, pp 401-409, May 1969.\n",
        "    Copyright   : (c) Dr Gavin C. Cawley, November 2007.\n",
        "    This program is free software; you can redistribute it and/or modify\n",
        "    it under the terms of the GNU General Public License as published by\n",
        "    the Free Software Foundation; either version 2 of the License, or\n",
        "    (at your option) any later version.\n",
        "    This program is distributed in the hope that it will be useful,\n",
        "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "    GNU General Public License for more details.\n",
        "    You should have received a copy of the GNU General Public License\n",
        "    along with this program; if not, write to the Free Software\n",
        "    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\n",
        "    \"\"\"\n",
        "\n",
        "    # Create distance matrix unless given by parameters\n",
        "    if inputdist == 'distance':\n",
        "        D = x\n",
        "        if init == 'default':\n",
        "            init = 'cmdscale'\n",
        "    else:\n",
        "        D = cdist(x, x)\n",
        "        if init == 'default':\n",
        "            init = 'pca'\n",
        "\n",
        "    if inputdist == 'distance' and init == 'pca':\n",
        "        raise ValueError(\"Cannot use init == 'pca' when inputdist == 'distance'\")\n",
        "\n",
        "    if np.count_nonzero(np.diagonal(D)) > 0:\n",
        "        raise ValueError(\"The diagonal of the dissimilarity matrix must be zero\")\n",
        "\n",
        "    # Remaining initialisation\n",
        "    N = x.shape[0]\n",
        "    scale = 0.5 / D.sum()\n",
        "    D = D + np.eye(N)     \n",
        "\n",
        "    if np.count_nonzero(D<=0) > 0:\n",
        "        raise ValueError(\"Off-diagonal dissimilarities must be strictly positive\")   \n",
        "\n",
        "    Dinv = 1 / D\n",
        "    if init == 'pca':\n",
        "        [UU,DD,_] = np.linalg.svd(x)\n",
        "        y = UU[:,:n]*DD[:n] \n",
        "    elif init == 'cmdscale':\n",
        "        from cmdscale import cmdscale\n",
        "        y,e = cmdscale(D)\n",
        "        y = y[:,:n]\n",
        "    else:\n",
        "        y = np.random.normal(0.0,1.0,[N,n])\n",
        "    one = np.ones([N,n])\n",
        "    d = cdist(y,y) + np.eye(N)\n",
        "    dinv = 1. / d\n",
        "    delta = D-d \n",
        "    E = ((delta**2)*Dinv).sum() \n",
        "\n",
        "    # Get on with it\n",
        "    for i in range(maxiter):\n",
        "        # Compute gradient, Hessian and search direction (note it is actually\n",
        "        # 1/4 of the gradient and Hessian, but the step size is just the ratio\n",
        "        # of the gradient and the diagonal of the Hessian so it doesn't\n",
        "        # matter).\n",
        "        delta = dinv - Dinv\n",
        "        deltaone = np.dot(delta,one)\n",
        "        g = np.dot(delta,y) - (y * deltaone)\n",
        "        dinv3 = dinv ** 3\n",
        "        y2 = y ** 2\n",
        "        H = np.dot(dinv3,y2) - deltaone - np.dot(2,y) * np.dot(dinv3,y) + y2 * np.dot(dinv3,one)\n",
        "        s = -g.flatten(order='F') / np.abs(H.flatten(order='F'))\n",
        "        y_old    = y\n",
        "\n",
        "        # Use step-halving procedure to ensure progress is made\n",
        "        for j in range(maxhalves):\n",
        "            s_reshape = np.reshape(s, (-1,n),order='F')\n",
        "            y = y_old + s_reshape\n",
        "            d = cdist(y, y) + np.eye(N)\n",
        "            dinv = 1 / d\n",
        "            delta = D - d\n",
        "            E_new = ((delta**2)*Dinv).sum()\n",
        "            if E_new < E:\n",
        "                break\n",
        "            else:\n",
        "                s = 0.5*s\n",
        "\n",
        "        # Bomb out if too many halving steps are required\n",
        "        if j == maxhalves-1:\n",
        "            print('Warning: maxhalves exceeded. Sammon mapping may not converge...')\n",
        "\n",
        "        # Evaluate termination criterion\n",
        "        if abs((E - E_new) / E) < tolfun:\n",
        "            if display:\n",
        "                print('TolFun exceeded: Optimisation terminated')\n",
        "            break\n",
        "\n",
        "        # Report progress\n",
        "        E = E_new\n",
        "        if display > 1:\n",
        "            print('epoch = %d : E = %12.10f'% (i+1, E * scale))\n",
        "\n",
        "    if i == maxiter-1:\n",
        "        print('Warning: maxiter exceeded. Sammon mapping may not have converged...')\n",
        "\n",
        "    # Fiddle stress to match the original Sammon paper\n",
        "    E = E * scale\n",
        "    \n",
        "    return [y,E]"
      ],
      "metadata": {
        "id": "DDQlmrPriOjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwwM5dIC8SCx"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import random \n",
        "import time\n",
        "from collections import defaultdict\n",
        "from scipy.stats.stats import pearsonr\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, roc_curve, auc, plot_confusion_matrix, roc_auc_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from PyNomaly import loop\n",
        "\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# evaluate model performance with outliers removed using isolation forest\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from pyod.models.knn import KNN   # kNN detector\n",
        "import pyod\n",
        "from pyod.utils.data import generate_data\n",
        "from pyod.utils.utility import score_to_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmSiOgLI5hlg"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlthKTNg9Xb9"
      },
      "source": [
        "# IMPORT DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_3HvJKu7_no",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "403f7d5a-141c-472b-c54c-8789dc6d8f0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
              "0  -3.676084   3.849076  11.387744   6.934825        0.0   178550.0   \n",
              "1  -0.054460   0.113233   0.000000   0.000000        0.0        0.0   \n",
              "2  -0.135713   0.206624   0.000000   0.000000        0.0        0.0   \n",
              "3   0.200376  -0.201457   0.000000   0.000000        0.0        0.0   \n",
              "4   0.185957  -0.145341   0.000000   0.000000        0.0        0.0   \n",
              "\n",
              "   feature_6    feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
              "0        0.0  1098.056338  23.586819        0.0    0.340280   -0.031884   \n",
              "1        0.0    15.946223  -0.038350        0.0   -1.175445   -4.559420   \n",
              "2        0.0     0.000000   4.715204        0.0   -0.810155   -2.489855   \n",
              "3        0.0     0.000000  -0.010384        0.0   -0.417582   -0.443478   \n",
              "4        0.0     0.000000   0.588176        0.0   -0.827586   -4.008696   \n",
              "\n",
              "   feature_12  feature_13  feature_14  feature_15  feature_16  class  \n",
              "0   -0.494523    0.000000         0.0     10728.0     10728.0      1  \n",
              "1   -0.782473    0.990215         0.0         0.0         0.0      1  \n",
              "2   -0.405321    0.324343         0.0      8799.0      5322.0      1  \n",
              "3    0.000000    0.000000         0.0         0.0         0.0      1  \n",
              "4   -0.423318    0.645396         0.0         0.0         0.0      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65a97b8b-8e98-4b6d-9dfe-27eede6119f0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>feature_11</th>\n",
              "      <th>feature_12</th>\n",
              "      <th>feature_13</th>\n",
              "      <th>feature_14</th>\n",
              "      <th>feature_15</th>\n",
              "      <th>feature_16</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-3.676084</td>\n",
              "      <td>3.849076</td>\n",
              "      <td>11.387744</td>\n",
              "      <td>6.934825</td>\n",
              "      <td>0.0</td>\n",
              "      <td>178550.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1098.056338</td>\n",
              "      <td>23.586819</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.340280</td>\n",
              "      <td>-0.031884</td>\n",
              "      <td>-0.494523</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10728.0</td>\n",
              "      <td>10728.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.054460</td>\n",
              "      <td>0.113233</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.946223</td>\n",
              "      <td>-0.038350</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.175445</td>\n",
              "      <td>-4.559420</td>\n",
              "      <td>-0.782473</td>\n",
              "      <td>0.990215</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.135713</td>\n",
              "      <td>0.206624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.715204</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.810155</td>\n",
              "      <td>-2.489855</td>\n",
              "      <td>-0.405321</td>\n",
              "      <td>0.324343</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8799.0</td>\n",
              "      <td>5322.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.200376</td>\n",
              "      <td>-0.201457</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.417582</td>\n",
              "      <td>-0.443478</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.185957</td>\n",
              "      <td>-0.145341</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.588176</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.827586</td>\n",
              "      <td>-4.008696</td>\n",
              "      <td>-0.423318</td>\n",
              "      <td>0.645396</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65a97b8b-8e98-4b6d-9dfe-27eede6119f0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-65a97b8b-8e98-4b6d-9dfe-27eede6119f0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-65a97b8b-8e98-4b6d-9dfe-27eede6119f0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "dataname = 'cardio.csv'\n",
        "\n",
        "data = pd.read_csv(dataname, delimiter=',') #load data\n",
        "data.drop('Unnamed: 0', inplace=True, axis=1)\n",
        "#data.drop('outlier', inplace=True, axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT0SnETRnbpo"
      },
      "outputs": [],
      "source": [
        "target = data['class']\n",
        "data.drop('class', inplace=True, axis=1) \n",
        "\n",
        "#supervised?\n",
        "#data.drop('outlier', inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FROID-LIGHT"
      ],
      "metadata": {
        "id": "dZh47Q2AlCtv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uMYCNPq5sp0"
      },
      "outputs": [],
      "source": [
        "scores_df = pd.DataFrame()\n",
        "binary_scores_df = pd.DataFrame()\n",
        "df_dimredu = pd.DataFrame()\n",
        "df_dataredu = pd.DataFrame()\n",
        "df_binredu = pd.DataFrame()\n",
        "\n",
        "from pyod.models.mcd import MCD\n",
        "from pyod.models.suod import SUOD\n",
        "from pyod.models.loda import LODA\n",
        "from pyod.models.feature_bagging import FeatureBagging\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.cblof import CBLOF\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.cof import COF\n",
        "from pyod.models.copod import COPOD\n",
        "from pyod.models.ecod import ECOD\n",
        "from pyod.models.sos import SOS\n",
        "from pyod.models.pca import PCA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import random_projection\n",
        "from sklearn.manifold import Isomap\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras import regularizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import KernelPCA\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn import manifold\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.ticker import NullFormatter\n",
        "\n",
        "def froid_light(data):\n",
        "\n",
        "  try:\n",
        "    print('EllipticEnvelope')\n",
        "    ee = EllipticEnvelope()\n",
        "    score = ee.fit_predict(data)\n",
        "    dec_func = ee.decision_function(data)\n",
        "    maha = ee.mahalanobis(data)\n",
        "    pred = ee.predict(data)\n",
        "    scores_df['elliptic_decfunc_original_cont0.001'] = dec_func\n",
        "    scores_df['elliptic_mahalanobis_original_cont0.001'] = maha\n",
        "    binary_scores_df['elliptic_env_original_binary_cont0.001'] = pred\n",
        "    binary_scores_df['elliptic_env_original_binary_cont0.001'] = np.where(binary_scores_df['elliptic_env_original_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Elliptic Envelope Config 1')\n",
        "\n",
        "  try:\n",
        "    print('IsolationForest')\n",
        "    #isolation forest\n",
        "    iso = IsolationForest(n_jobs=-1)\n",
        "    #returns -1 for outliers and +1 for inliers\n",
        "    pred = iso.fit_predict(data)\n",
        "    #using decision function i can obtain outlierness for each observation\n",
        "    sklearn_score_anomalies = iso.decision_function(data)\n",
        "    original_paper_score = [-1*s + 0.5 for s in sklearn_score_anomalies]\n",
        "    scores_df['iso_forest_paper_score_original_cont0.001'] = original_paper_score\n",
        "    binary_scores_df['iso_forest_paper_score_original_binary_cont0.001'] = pred\n",
        "    binary_scores_df['iso_forest_paper_score_original_binary_cont0.001'] = np.where(binary_scores_df['iso_forest_paper_score_original_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Iso Forest Config 1')\n",
        "\n",
        "  try:\n",
        "    print('LOF')\n",
        "    #Local Outlier Factor\n",
        "    lof = LocalOutlierFactor(novelty=True, metric='minkowski',n_jobs=-1)\n",
        "    lof.fit(data)\n",
        "    lof_score = lof.decision_function(data)\n",
        "    pred = lof.predict(data)\n",
        "    scores_df['lof_score_original_mink_nei10'] = lof_score\n",
        "    binary_scores_df['lof_score_original_binary_mink_nei10'] = pred\n",
        "    binary_scores_df['lof_score_original_binary_mink_nei10'] = np.where(binary_scores_df['lof_score_original_binary_mink_nei10']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> LOF Config 1')\n",
        "\n",
        "  try:\n",
        "    print('COPOD')\n",
        "    clf = COPOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_copod = clf.decision_scores_  # raw outlier scores on the train data\n",
        "    scores_df['pyod_copod_original_cont0.001'] = pyod_copod\n",
        "    binary_scores_df['pyod_copod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> COPOD Config 1')\n",
        "\n",
        "  try:\n",
        "    print('ECOD')\n",
        "    clf = ECOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_ecod = clf.decision_scores_\n",
        "    scores_df['pyod_ecod_original_cont0.001'] = pyod_ecod\n",
        "    binary_scores_df['pyod_ecod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> ECOD Config 1')\n",
        "\n",
        "  try:\n",
        "    print('PCA')\n",
        "    clf = PCA(n_components=2)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_pca = clf.decision_scores_\n",
        "    scores_df['pyod_pca_original_cont0.001_whitFalse'] = pyod_pca\n",
        "    binary_scores_df['pyod_pca_original_binary_cont0.001_whitFalse'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> PCA Config 1 data maybe not converged')\n",
        "\n",
        "  try:\n",
        "    print('MCD')\n",
        "    clf = MCD()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_mcd = clf.decision_scores_\n",
        "    scores_df['pyod_mcd_original_cont0.001'] = pyod_mcd\n",
        "    binary_scores_df['pyod_mcd_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> MCD Config 1')\n",
        "\n",
        "  try:\n",
        "    print('COF')\n",
        "    clf = COF()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cof = clf.decision_scores_\n",
        "    scores_df['pyod_cof_original_cont0.001_nei10'] = pyod_cof\n",
        "    binary_scores_df['pyod_cof_original_binary_cont0.001_nei10'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> COF Config 1')\n",
        "\n",
        "  try:\n",
        "    print('CBLOF')\n",
        "    clf = CBLOF(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cblof = clf.decision_scores_\n",
        "    scores_df['pyod_cblof_original_cont0.001'] = pyod_cblof\n",
        "    binary_scores_df['pyod_cblof_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> CBLOF Config 1')\n",
        "\n",
        "  try:\n",
        "    print('HBOS')\n",
        "    clf = HBOS()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_hbos = clf.decision_scores_\n",
        "    scores_df['pyod_hbos_original_cont0.001'] = pyod_hbos\n",
        "    binary_scores_df['pyod_hbos_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> HBOS Config 1')\n",
        "\n",
        "  try:\n",
        "    print('KNN')\n",
        "    clf = KNN(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_knn = clf.decision_scores_\n",
        "    scores_df['pyod_knn_original_cont0.001'] = pyod_knn\n",
        "    binary_scores_df['pyod_knn_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> KNN Config 1')\n",
        "\n",
        "  try:\n",
        "    print('FeatureBagging')\n",
        "    clf = FeatureBagging(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_featbagg = clf.decision_scores_\n",
        "    scores_df['pyod_featbagg_original_cont0.001'] = pyod_featbagg\n",
        "    binary_scores_df['pyod_featbagg_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> Feature Bagging Config 1')\n",
        "\n",
        "  try:\n",
        "    print('LODA')\n",
        "    clf = LODA()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_loda = clf.decision_scores_\n",
        "    scores_df['pyod_loda_original_cont0.001'] = pyod_loda\n",
        "    binary_scores_df['pyod_loda_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> LODA Config 1')\n",
        "\n",
        "  try:\n",
        "    print('SUOD')\n",
        "    clf = SUOD()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_suod = clf.decision_scores_\n",
        "    scores_df['pyod_suod_original_cont0.001'] = pyod_suod\n",
        "    binary_scores_df['pyod_suod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> SUOD Config 1')\n",
        "\n",
        "  print('')\n",
        "  print('Dimensionality Reduction Algorithms: ')\n",
        "  print('PCA')\n",
        "  pca = PCA(n_components=2)\n",
        "  out_pca = pd.DataFrame()\n",
        "  out_pca_scores = pd.DataFrame()\n",
        "  out_pca_bin = pd.DataFrame()\n",
        "  try:\n",
        "    pca.fit(data)\n",
        "    out_pca = pd.DataFrame(pca.transform(data), index=data.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 1')\n",
        "\n",
        "  try:\n",
        "    pca.fit(scores_df)\n",
        "    out_pca_scores = pd.DataFrame(pca.transform(scores_df), index=scores_df.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 2')\n",
        "\n",
        "  try:\n",
        "    pca.fit(binary_scores_df)\n",
        "    out_pca_bin = pd.DataFrame(pca.transform(binary_scores_df), index=binary_scores_df.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 3')\n",
        "\n",
        "  try:\n",
        "    df_dataredu['pca1_data'] = out_pca[0]\n",
        "    df_dataredu['pca2_data'] = out_pca[1]\n",
        "    df_dimredu['pca1_scores'] = out_pca_scores[0]\n",
        "    df_dimredu['pca2_scores'] = out_pca_scores[1]\n",
        "    df_binredu['pca1_binary'] = out_pca_bin[0]\n",
        "    df_binredu['pca2_binary'] = out_pca_bin[1]\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config loading')\n",
        "\n",
        "  print('RandomGaussianProjection')\n",
        "  rsp = random_projection.GaussianRandomProjection(n_components=2)\n",
        "  try:\n",
        "    ran_proj = rsp.fit_transform(scores_df)\n",
        "    df_dimredu['sub_proj1_scores'] = ran_proj[:,0]\n",
        "    df_dimredu['sub_proj2_scores'] = ran_proj[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config scores')\n",
        "\n",
        "  try:  \n",
        "    ran_proj_data = rsp.fit_transform(data)\n",
        "    df_dataredu['sub_proj1_data'] = ran_proj_data[:,0]\n",
        "    df_dataredu['sub_proj2_data'] = ran_proj_data[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config data')\n",
        "\n",
        "  try:\n",
        "    ran_proj_bin = rsp.fit_transform(binary_scores_df)\n",
        "    df_binredu['sub_proj1_binary'] = ran_proj_bin[:,0]\n",
        "    df_binredu['sub_proj2_binary'] = ran_proj_bin[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config binary')\n",
        "\n",
        "  print('TSNE')\n",
        "  tsne = TSNE(n_components=2)\n",
        "\n",
        "  try:\n",
        "    sne_proj = tsne.fit_transform(scores_df)\n",
        "    df_dimredu['sne_1_scores'] = sne_proj[:,0]\n",
        "    df_dimredu['sne_2_scores'] = sne_proj[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config scores')\n",
        "\n",
        "  try:  \n",
        "    sne_proj_data = tsne.fit_transform(data)\n",
        "    df_dataredu['sne_1_data'] = sne_proj_data[:,0]\n",
        "    df_dataredu['sne_2_data'] = sne_proj_data[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config data')\n",
        "\n",
        "  try:\n",
        "    sne_proj_bin = tsne.fit_transform(binary_scores_df)\n",
        "    df_binredu['sne_1_binary'] = sne_proj_bin[:,0]\n",
        "    df_binredu['sne_2_binary'] = sne_proj_bin[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config binary')\n",
        "\n",
        "  print('LinearDiscriminantAnalysis')\n",
        "  clf = LinearDiscriminantAnalysis()\n",
        "\n",
        "  try:\n",
        "    clf.fit(scores_df, target)\n",
        "    lda_proj = clf.transform(scores_df)\n",
        "    df_dimredu['lda_1_scores_0'] = lda_proj[0:lda_proj.shape[0], 0]\n",
        "    df_dimredu['lda_1_scores_1'] = lda_proj[0:lda_proj.shape[0], 1]\n",
        "    df_dimredu['lda_1_scores_2'] = lda_proj[0:lda_proj.shape[0], 2]\n",
        "    df_dimredu['lda_1_scores_3'] = lda_proj[0:lda_proj.shape[0], 3]\n",
        "    df_dimredu['lda_1_scores_4'] = lda_proj[0:lda_proj.shape[0], 4]\n",
        "    df_dimredu['lda_1_scores_5'] = lda_proj[0:lda_proj.shape[0], 5]\n",
        "    df_dimredu['lda_1_scores_6'] = lda_proj[0:lda_proj.shape[0], 6]\n",
        "    df_dimredu['lda_1_scores_7'] = lda_proj[0:lda_proj.shape[0], 7]\n",
        "    df_dimredu['lda_1_scores_8'] = lda_proj[0:lda_proj.shape[0], 8]\n",
        "    df_dimredu['lda_1_scores_9'] = lda_proj[0:lda_proj.shape[0], 9]\n",
        "    df_dimredu['lda_1_scores_10'] = lda_proj[0:lda_proj.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis (classes exceeded)')\n",
        "\n",
        "  try:\n",
        "    clf.fit(data, target)\n",
        "    lda_proj_data = clf.transform(data)\n",
        "    df_dataredu['lda_1_data_0'] = lda_proj_data[0:lda_proj_data.shape[0], 0]\n",
        "    df_dataredu['lda_1_data_1'] = lda_proj_data[0:lda_proj_data.shape[0], 1]\n",
        "    df_dataredu['lda_1_data_2'] = lda_proj_data[0:lda_proj_data.shape[0], 2]\n",
        "    df_dataredu['lda_1_data_3'] = lda_proj_data[0:lda_proj_data.shape[0], 3]\n",
        "    df_dataredu['lda_1_data_4'] = lda_proj_data[0:lda_proj_data.shape[0], 4]\n",
        "    df_dataredu['lda_1_data_5'] = lda_proj_data[0:lda_proj_data.shape[0], 5]\n",
        "    df_dataredu['lda_1_data_6'] = lda_proj_data[0:lda_proj_data.shape[0], 6]\n",
        "    df_dataredu['lda_1_data_7'] = lda_proj_data[0:lda_proj_data.shape[0], 7]\n",
        "    df_dataredu['lda_1_data_8'] = lda_proj_data[0:lda_proj_data.shape[0], 8]\n",
        "    df_dataredu['lda_1_data_9'] = lda_proj_data[0:lda_proj_data.shape[0], 9]\n",
        "    df_dataredu['lda_1_data_10'] = lda_proj_data[0:lda_proj_data.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)')\n",
        "\n",
        "  try:\n",
        "    clf.fit(binary_scores_df, target)\n",
        "    lda_proj_bin = clf.transform(binary_scores_df) \n",
        "    df_binredu['lda_1_binary_0'] = lda_proj_bin[0:lda_proj_bin.shape[0], 0]\n",
        "    df_binredu['lda_1_binary_1'] = lda_proj_bin[0:lda_proj_bin.shape[0], 1]\n",
        "    df_binredu['lda_1_binary_2'] = lda_proj_bin[0:lda_proj_bin.shape[0], 2]\n",
        "    df_binredu['lda_1_binary_3'] = lda_proj_bin[0:lda_proj_bin.shape[0], 3]\n",
        "    df_binredu['lda_1_binary_4'] = lda_proj_bin[0:lda_proj_bin.shape[0], 4]\n",
        "    df_binredu['lda_1_binary_5'] = lda_proj_bin[0:lda_proj_bin.shape[0], 5]\n",
        "    df_binredu['lda_1_binary_6'] = lda_proj_bin[0:lda_proj_bin.shape[0], 6]\n",
        "    df_binredu['lda_1_binary_7'] = lda_proj_bin[0:lda_proj_bin.shape[0], 7]\n",
        "    df_binredu['lda_1_binary_8'] = lda_proj_bin[0:lda_proj_bin.shape[0], 8]\n",
        "    df_binredu['lda_1_binary_9'] = lda_proj_bin[0:lda_proj_bin.shape[0], 9]\n",
        "    df_binredu['lda_1_binary_10'] = lda_proj_bin[0:lda_proj_bin.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H9dBtOaOpyP"
      },
      "source": [
        "#APPLY FEATURE EXTRACTION PIPELINE ON ORIGINAL DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1UTkoUYKOi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0fac27e-2c62-45e0-e7c1-f9c4a3af8b39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Extraction: \n",
            "\n",
            "EllipticEnvelope\n",
            "IsolationForest\n",
            "LOF\n",
            "COPOD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    4.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    4.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECOD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.2s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA\n",
            "Exception Raised -> PCA Config 1 data maybe not converged\n",
            "MCD\n",
            "COF\n",
            "CBLOF\n",
            "HBOS\n",
            "KNN\n",
            "FeatureBagging\n",
            "LODA\n",
            "SUOD\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimensionality Reduction Algorithms: \n",
            "PCA\n",
            "Exception Raised --> PCA Config 2\n",
            "Exception Raised --> PCA Config loading\n",
            "RandomGaussianProjection\n",
            "TSNE\n",
            "LinearDiscriminantAnalysis\n",
            "Exception Raised --> Linear Discriminant Analysis (classes exceeded)\n",
            "Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)\n",
            "Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)\n",
            "Support Features Extraction Time: 442.3586 seconds\n",
            "\n",
            "Null Values in Outlierness Scores DataSets:\n",
            "Scores_df: \n",
            "--- --- ---\n",
            "Binary_Scores_df: \n",
            "--- --- ---\n",
            "\n",
            "Null Values in Dimensionality Reduction DataSets:\n",
            "sub_proj1_scores    0\n",
            "sub_proj2_scores    0\n",
            "sne_1_scores        0\n",
            "sne_2_scores        0\n",
            "dtype: int64\n",
            "--- --- ---\n",
            "pca1_data         0\n",
            "pca2_data         0\n",
            "sub_proj1_data    0\n",
            "sub_proj2_data    0\n",
            "sne_1_data        0\n",
            "sne_2_data        0\n",
            "lda_1_data_0      0\n",
            "dtype: int64\n",
            "--- --- ---\n",
            "sub_proj1_binary    0\n",
            "sub_proj2_binary    0\n",
            "sne_1_binary        0\n",
            "sne_2_binary        0\n",
            "lda_1_binary_0      0\n",
            "dtype: int64\n",
            "--- --- ---\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print('Feature Extraction: \\n')\n",
        "froid_light(data)\n",
        "\n",
        "end = time.time()\n",
        "#save first feature extraction phase time ellapsed\n",
        "print(\"Support Features Extraction Time: %.8s seconds\" % (end - start_time))\n",
        "\n",
        "# --- --- --- --- --- #\n",
        "\n",
        "print('\\nNull Values in Outlierness Scores DataSets:')\n",
        "print('Scores_df: ')\n",
        "scores_df.loc[:, scores_df.isna().any()]\n",
        "scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "print('--- --- ---')\n",
        "print('Binary_Scores_df: ')\n",
        "binary_scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "binary_scores_df.loc[:, binary_scores_df.isna().any()]\n",
        "print('--- --- ---')\n",
        "print('\\nNull Values in Dimensionality Reduction DataSets:')\n",
        "df_dimredu.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "print(df_dimredu.isnull().sum())\n",
        "print('--- --- ---')\n",
        "df_dataredu.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "print(df_dataredu.isnull().sum())\n",
        "print('--- --- ---')\n",
        "df_binredu.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "print(df_binredu.isnull().sum())\n",
        "print('--- --- ---')\n",
        "\n",
        "# --- --- --- --- --- #\n",
        "\n",
        "#normalize outlierness scores\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "scores_df = pd.DataFrame(scaler.fit_transform(scores_df), columns=scores_df.columns)\n",
        "scores_df.to_csv( \"outlierness_scores_light.csv\")\n",
        "binary_scores_df.to_csv(\"binary_scores_light.csv\")\n",
        "\n",
        "df_dimredu = pd.DataFrame(scaler.fit_transform(df_dimredu), columns=df_dimredu.columns)\n",
        "df_dataredu = pd.DataFrame(scaler.fit_transform(df_dataredu), columns=df_dataredu.columns)\n",
        "df_binredu = pd.DataFrame(scaler.fit_transform(df_binredu), columns=df_binredu.columns)\n",
        "df_dimredu.to_csv( \"dim_redu_scores_light.csv\")\n",
        "df_dataredu.to_csv(\"dim_redu_original_data_light.csv\")\n",
        "df_binredu.to_csv(\"dim_redu_binary_scores_light.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-1PW4MHDATY"
      },
      "source": [
        "#OUTLIERNESS SCORES EXTRACTION ON PRINCIPAL COMPONENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJJZzXZiDTJg"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('dim_redu_original_data_light.csv', delimiter=',') #load data\n",
        "data.drop('Unnamed: 0', inplace=True, axis=1)\n",
        "\n",
        "original = pd.read_csv(dataname, delimiter=',') #load data\n",
        "original.drop('Unnamed: 0', inplace=True, axis=1)\n",
        "target = original['class']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-5r-qRIDATY"
      },
      "source": [
        "##pynomaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfi_KgeL0x1n"
      },
      "outputs": [],
      "source": [
        "scores_df = pd.DataFrame()\n",
        "binary_scores_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bOjTQ72DATZ"
      },
      "source": [
        "##sklearn algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4ZpafaA1IPm"
      },
      "outputs": [],
      "source": [
        "from pyod.models.copod import COPOD\n",
        "from pyod.models.suod import SUOD\n",
        "from pyod.models.loda import LODA\n",
        "from pyod.models.feature_bagging import FeatureBagging\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.cblof import CBLOF\n",
        "from pyod.models.cof import COF\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.mcd import MCD\n",
        "from pyod.models.pca import PCA\n",
        "from pyod.models.ecod import ECOD\n",
        "from pyod.models.sos import SOS\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "def froid_light(data):\n",
        "  try:\n",
        "    #Minimum Covariance Determinant\n",
        "    ee = EllipticEnvelope()\n",
        "    score = ee.fit_predict(data)\n",
        "    dec_func = ee.decision_function(data)\n",
        "    maha = ee.mahalanobis(data)\n",
        "    pred = ee.predict(data)\n",
        "    scores_df['elliptic_decfunc_dim_redu_cont0.001'] = dec_func\n",
        "    scores_df['elliptic_mahalanobis_dim_redu_cont0.001'] = maha\n",
        "    binary_scores_df['elliptic_env_dim_redu_binary_cont0.001'] = pred\n",
        "    binary_scores_df['elliptic_env_dim_redu_binary_cont0.001'] = np.where(binary_scores_df['elliptic_env_dim_redu_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Elliptic Envelope Config 1')\n",
        "\n",
        "  try:\n",
        "    #isolation forest\n",
        "    iso = IsolationForest(n_jobs=-1)\n",
        "    #returns -1 for outliers and +1 for inliers\n",
        "    pred = iso.fit_predict(data)\n",
        "    #using decision function i can obtain outlierness for each observation\n",
        "    sklearn_score_anomalies = iso.decision_function(data)\n",
        "    original_paper_score = [-1*s + 0.5 for s in sklearn_score_anomalies]\n",
        "    scores_df['iso_forest_paper_score_dim_redu_cont0.001'] = original_paper_score\n",
        "    binary_scores_df['iso_forest_paper_score_dim_redu_binary_cont0.001'] = pred\n",
        "    binary_scores_df['iso_forest_paper_score_dim_redu_binary_cont0.001'] = np.where(binary_scores_df['iso_forest_paper_score_dim_redu_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Isolation Forest Config 1')\n",
        "\n",
        "  try:\n",
        "    #Local Outlier Factor\n",
        "    lof = LocalOutlierFactor(novelty=True, metric='minkowski',n_jobs=-1)\n",
        "    lof.fit(data)\n",
        "    lof_score = lof.decision_function(data)\n",
        "    pred = lof.predict(data)\n",
        "    scores_df['lof_score_dim_redu_mink_nei10'] = lof_score\n",
        "    binary_scores_df['lof_score_dim_redu_binary_mink_nei10'] = pred\n",
        "    binary_scores_df['lof_score_dim_redu_binary_mink_nei10'] = np.where(binary_scores_df['lof_score_dim_redu_binary_mink_nei10']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> LOF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = COPOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_copod = clf.decision_scores_  # raw outlier scores on the train data\n",
        "    scores_df['pyod_copod_dim_redu_cont0.001'] = pyod_copod\n",
        "    binary_scores_df['pyod_copod_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> COPOD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = ECOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_ecod = clf.decision_scores_\n",
        "    scores_df['pyod_ecod_dim_redu_cont0.001'] = pyod_ecod\n",
        "    binary_scores_df['pyod_ecod_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> ECOD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = PCA(n_components=2,whiten=False)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_pca = clf.decision_scores_\n",
        "    scores_df['pyod_pca_dim_redu_cont0.001_whitFalse'] = pyod_pca\n",
        "    binary_scores_df['pyod_pca_dim_redu_binary_cont0.001_whitFalse'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = MCD()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_mcd = clf.decision_scores_\n",
        "    scores_df['pyod_mcd_dim_redu_cont0.001'] = pyod_mcd\n",
        "    binary_scores_df['pyod_mcd_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> MCD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = COF()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cof = clf.decision_scores_\n",
        "    scores_df['pyod_cof_dim_redu_cont0.001_nei10'] = pyod_cof\n",
        "    binary_scores_df['pyod_cof_dim_redu_binary_cont0.001_nei10'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> COF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = CBLOF(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cblof = clf.decision_scores_\n",
        "    scores_df['pyod_cblof_dim_redu_cont0.001'] = pyod_cblof\n",
        "    binary_scores_df['pyod_cblof_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> CBLOF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = HBOS()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_hbos = clf.decision_scores_\n",
        "    scores_df['pyod_hbos_dim_redu_cont0.001'] = pyod_hbos\n",
        "    binary_scores_df['pyod_hbos_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> HBOS Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = KNN(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_knn = clf.decision_scores_\n",
        "    scores_df['pyod_knn_dim_redu_cont0.001'] = pyod_knn\n",
        "    binary_scores_df['pyod_knn_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> KNN Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = FeatureBagging(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_featbagg = clf.decision_scores_\n",
        "    scores_df['pyod_featbagg_dim_redu_cont0.001'] = pyod_featbagg\n",
        "    binary_scores_df['pyod_featbagg_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> Feature Bagging Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = LODA()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_loda = clf.decision_scores_\n",
        "    scores_df['pyod_loda_dim_redu_cont0.001'] = pyod_loda\n",
        "    binary_scores_df['pyod_loda_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> LODA Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = SUOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_suod = clf.decision_scores_\n",
        "    scores_df['pyod_suod_dim_redu_cont0.001'] = pyod_suod\n",
        "    binary_scores_df['pyod_suod_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> SUOD Config 1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0h5MIoF35Zs"
      },
      "source": [
        "#APPLY FEATURE EXTRACTION PIPELINE ON PRINCIPAL COMPONENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tffutDv4BN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dec5be0-6f77-4245-8f12-a78f235b2ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outlierness Scores from Principal Components Datasets: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    2.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    2.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    3.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    3.4s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    7.7s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    7.7s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    5.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    5.0s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Features on Principal Components Extraction Time: 101.9488 seconds\n",
            "\n",
            "Null Values in Outlierness Scores DataSets:\n",
            "Scores_df: \n",
            "--- --- ---\n",
            "Binary_Scores_df: \n",
            "--- --- ---\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "import time\n",
        "\n",
        "print('Outlierness Scores from Principal Components Datasets: ')\n",
        "start_time = time.time()\n",
        "froid_light(data)\n",
        "end = time.time()\n",
        "\n",
        "#save first feature extraction phase time ellapsed\n",
        "print(\"Support Features on Principal Components Extraction Time: %.8s seconds\" % (end - start_time))\n",
        "print('\\nNull Values in Outlierness Scores DataSets:')\n",
        "print('Scores_df: ')\n",
        "scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "scores_df.loc[:, scores_df.isna().any()]\n",
        "print('--- --- ---')\n",
        "print('Binary_Scores_df: ')\n",
        "binary_scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "binary_scores_df.loc[:, binary_scores_df.isna().any()]\n",
        "print('--- --- ---')\n",
        "\n",
        "#normalize outlierness scores\n",
        "scaler = RobustScaler()\n",
        "scores_df = pd.DataFrame(scaler.fit_transform(scores_df), columns=scores_df.columns)\n",
        "\n",
        "scores_df.to_csv( \"scores_principal_comp_light.csv\")\n",
        "binary_scores_df.to_csv( \"binary_scores_principal_comp_light.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKjSFOGnDvyt"
      },
      "source": [
        "# INCEPTION FROID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrInPSL6DoGA"
      },
      "outputs": [],
      "source": [
        "dataname = 'outlierness_scores_light.csv'\n",
        "dataname2 = 'dim_redu_original_data_light.csv'\n",
        "\n",
        "data = pd.read_csv(dataname, delimiter=',') #load outlierness scores on original data -> apply anomaly detection on it\n",
        "data.drop('Unnamed: 0', inplace=True, axis=1)\n",
        "\n",
        "data2 = pd.read_csv(dataname2, delimiter=',') #load principal components on original data -> apply dimensionality reduction on it\n",
        "data2.drop('Unnamed: 0', inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvkxWvmiDvyt"
      },
      "outputs": [],
      "source": [
        "scores_df = pd.DataFrame()\n",
        "binary_scores_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCF1tx7TDvyz"
      },
      "outputs": [],
      "source": [
        "from pyod.models.mcd import MCD\n",
        "from pyod.models.suod import SUOD\n",
        "from pyod.models.loda import LODA\n",
        "from pyod.models.feature_bagging import FeatureBagging\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.cblof import CBLOF\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.cof import COF\n",
        "from pyod.models.copod import COPOD\n",
        "from pyod.models.ecod import ECOD\n",
        "from pyod.models.sos import SOS\n",
        "from pyod.models.pca import PCA\n",
        "\n",
        "def froid_light(data):\n",
        "  #Minimum Covariance Determinant\n",
        "  try:\n",
        "    ee = EllipticEnvelope()\n",
        "    score = ee.fit_predict(data)\n",
        "    dec_func = ee.decision_function(data)\n",
        "    maha = ee.mahalanobis(data)\n",
        "    pred = ee.predict(data)\n",
        "    scores_df['inception_elliptic_decfunc_original_cont0.001'] = dec_func\n",
        "    scores_df['inception_elliptic_mahalanobis_original_cont0.001'] = maha\n",
        "    binary_scores_df['inception_elliptic_env_original_binary_cont0.001'] = pred\n",
        "    binary_scores_df['inception_elliptic_env_original_binary_cont0.001'] = np.where(binary_scores_df['inception_elliptic_env_original_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Elliptic Envelope Config 1')\n",
        "\n",
        "  try:\n",
        "    #isolation forest\n",
        "    iso = IsolationForest(n_jobs=-1)\n",
        "    #returns -1 for outliers and +1 for inliers\n",
        "    pred = iso.fit_predict(data)\n",
        "    #using decision function i can obtain outlierness for each observation\n",
        "    sklearn_score_anomalies = iso.decision_function(data)\n",
        "    original_paper_score = [-1*s + 0.5 for s in sklearn_score_anomalies]\n",
        "    scores_df['inception_iso_forest_paper_score_original_cont0.001'] = original_paper_score\n",
        "    binary_scores_df['inception_iso_forest_paper_score_original_binary_cont0.001'] = pred\n",
        "    binary_scores_df['inception_iso_forest_paper_score_original_binary_cont0.001'] = np.where(binary_scores_df['inception_iso_forest_paper_score_original_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Iso Forest Config 1')\n",
        "\n",
        "  try:\n",
        "    #Local Outlier Factor\n",
        "    lof = LocalOutlierFactor(novelty=True, metric='minkowski',n_jobs=-1)\n",
        "    lof.fit(data)\n",
        "    lof_score = lof.decision_function(data)\n",
        "    pred = lof.predict(data)\n",
        "    scores_df['inception_lof_score_original_mink_nei10'] = lof_score\n",
        "    binary_scores_df['inception_lof_score_original_binary_mink_nei10'] = pred\n",
        "    binary_scores_df['inception_lof_score_original_binary_mink_nei10'] = np.where(binary_scores_df['inception_lof_score_original_binary_mink_nei10']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> LOF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = COPOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_copod = clf.decision_scores_  # raw outlier scores on the train data\n",
        "    scores_df['inception_pyod_copod_original_cont0.001'] = pyod_copod\n",
        "    binary_scores_df['inception_pyod_copod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> COPOD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = ECOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_ecod = clf.decision_scores_\n",
        "    scores_df['inception_pyod_ecod_original_cont0.001'] = pyod_ecod\n",
        "    binary_scores_df['inception_pyod_ecod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> ECOD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = PCA(n_components=2)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_pca = clf.decision_scores_\n",
        "    scores_df['inception_pyod_pca_original_cont0.001_whitFalse'] = pyod_pca\n",
        "    binary_scores_df['inception_pyod_pca_original_binary_cont0.001_whitFalse'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> PCA Config 1 data maybe not converged')\n",
        "\n",
        "  try:\n",
        "    clf = MCD()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_mcd = clf.decision_scores_\n",
        "    scores_df['inception_pyod_mcd_original_cont0.001'] = pyod_mcd\n",
        "    binary_scores_df['inception_pyod_mcd_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> MCD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = COF()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cof = clf.decision_scores_\n",
        "    scores_df['inception_pyod_cof_original_cont0.001_nei10'] = pyod_cof\n",
        "    binary_scores_df['inception_pyod_cof_original_binary_cont0.001_nei10'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> COF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = CBLOF(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cblof = clf.decision_scores_\n",
        "    scores_df['inception_pyod_cblof_original_cont0.001'] = pyod_cblof\n",
        "    binary_scores_df['inception_pyod_cblof_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> CBLOF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = HBOS()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_hbos = clf.decision_scores_\n",
        "    scores_df['inception_pyod_hbos_original_cont0.001'] = pyod_hbos\n",
        "    binary_scores_df['inception_pyod_hbos_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> HBOS Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = KNN(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_knn = clf.decision_scores_\n",
        "    scores_df['inception_pyod_knn_original_cont0.001'] = pyod_knn\n",
        "    binary_scores_df['inception_pyod_knn_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> KNN Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = FeatureBagging(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_featbagg = clf.decision_scores_\n",
        "    scores_df['inception_pyod_featbagg_original_cont0.001'] = pyod_featbagg\n",
        "    binary_scores_df['inception_pyod_featbagg_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> Feature Bagging Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = LODA()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_loda = clf.decision_scores_\n",
        "    scores_df['inception_pyod_loda_original_cont0.001'] = pyod_loda\n",
        "    binary_scores_df['inception_pyod_loda_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> LODA Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = SUOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_suod = clf.decision_scores_\n",
        "    scores_df['inception_pyod_suod_original_cont0.001'] = pyod_suod\n",
        "    binary_scores_df['inception_pyod_suod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> SUOD Config 1')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "VwMoaw2L2069",
        "outputId": "d9e0e604-f571-4857-f2c3-ec8e851956fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0136d79f-07a0-475c-988a-d5472c1e52ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0136d79f-07a0-475c-988a-d5472c1e52ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0136d79f-07a0-475c-988a-d5472c1e52ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0136d79f-07a0-475c-988a-d5472c1e52ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print('Outlierness Scores Extraction: \\n')\n",
        "froid_light(data) #anomaly detection methods applied on outlierness scores\n",
        "\n",
        "#normalize outlierness scores\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "scores_df = pd.DataFrame(scaler.fit_transform(scores_df), columns=scores_df.columns)\n",
        "\n",
        "print('\\nNull Values in Outlierness Scores DataSets:')\n",
        "print('Scores_df: ')\n",
        "scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "scores_df.loc[:, scores_df.isna().any()]\n",
        "print('--- --- ---')\n",
        "print('Binary_Scores_df: ')\n",
        "binary_scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "binary_scores_df.loc[:, binary_scores_df.isna().any()]\n",
        "print('--- --- ---')\n",
        "\n",
        "#save outlierness numeric and binary scores to .csv\n",
        "scores_df.to_csv( \"inception_outlierness_scores_light.csv\")\n",
        "binary_scores_df.to_csv( \"inception_binary_scores_light.csv\")"
      ],
      "metadata": {
        "id": "-0ye8qQNdW0N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "a27ddf34-af8c-497a-c482-8de231e2d063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outlierness Scores Extraction: \n",
            "\n",
            "Exception Raised --> Elliptic Envelope Config 1\n",
            "Exception Raised --> Iso Forest Config 1\n",
            "Exception Raised --> LOF Config 1\n",
            "Exception Raised -> COPOD Config 1\n",
            "Exception Raised -> ECOD Config 1\n",
            "Exception Raised -> PCA Config 1 data maybe not converged\n",
            "Exception Raised -> MCD Config 1\n",
            "Exception Raised -> COF Config 1\n",
            "Exception Raised -> CBLOF Config 1\n",
            "Exception Raised -> HBOS Config 1\n",
            "Exception Raised -> KNN Config 1\n",
            "Exception Raised -> Feature Bagging Config 1\n",
            "Exception Raised -> LODA Config 1\n",
            "Exception Raised -> SUOD Config 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-9f7846d634d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mscores_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscores_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nNull Values in Outlierness Scores DataSets:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1493\u001b[0;31m             \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1494\u001b[0m         )\n\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw04x-ufD1vU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import random_projection\n",
        "from sklearn.manifold import Isomap\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras import regularizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import KernelPCA\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn import manifold\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.ticker import NullFormatter\n",
        "\n",
        "df_dimredu = pd.DataFrame()\n",
        "df_dataredu = pd.DataFrame()\n",
        "df_binredu = pd.DataFrame()\n",
        "\n",
        "def froid_light(data):\n",
        "  pca = PCA(n_components=2)\n",
        "  out_pca = pd.DataFrame()\n",
        "  out_pca_scores = pd.DataFrame()\n",
        "  out_pca_bin = pd.DataFrame()\n",
        "  try:\n",
        "    pca.fit(data)\n",
        "    out_pca = pd.DataFrame(pca.transform(data), index=data.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 1')\n",
        "\n",
        "  try:\n",
        "    pca.fit(scores_df)\n",
        "    out_pca_scores = pd.DataFrame(pca.transform(scores_df), index=scores_df.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 2')\n",
        "\n",
        "  try:\n",
        "    pca.fit(binary_scores_df)\n",
        "    out_pca_bin = pd.DataFrame(pca.transform(binary_scores_df), index=binary_scores_df.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 3')\n",
        "\n",
        "  try:\n",
        "    df_dataredu['inception_pca1_data'] = out_pca[0]\n",
        "    df_dataredu['inception_pca2_data'] = out_pca[1]\n",
        "    df_dimredu['inception_pca1_scores'] = out_pca_scores[0]\n",
        "    df_dimredu['inception_pca2_scores'] = out_pca_scores[1]\n",
        "    df_binredu['inception_pca1_binary'] = out_pca_bin[0]\n",
        "    df_binredu['inception_pca2_binary'] = out_pca_bin[1]\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config loading')\n",
        "\n",
        "  rsp = random_projection.GaussianRandomProjection(n_components=2)\n",
        "  try:\n",
        "    ran_proj = rsp.fit_transform(scores_df)\n",
        "    df_dimredu['inception_sub_proj1_scores'] = ran_proj[:,0]\n",
        "    df_dimredu['inception_sub_proj2_scores'] = ran_proj[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config scores')\n",
        "\n",
        "  try:  \n",
        "    ran_proj_data = rsp.fit_transform(data)\n",
        "    df_dataredu['inception_sub_proj1_data'] = ran_proj_data[:,0]\n",
        "    df_dataredu['inception_sub_proj2_data'] = ran_proj_data[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config data')\n",
        "\n",
        "  try:\n",
        "    ran_proj_bin = rsp.fit_transform(binary_scores_df)\n",
        "    df_binredu['inception_sub_proj1_binary'] = ran_proj_bin[:,0]\n",
        "    df_binredu['inception_sub_proj2_binary'] = ran_proj_bin[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config binary')\n",
        "\n",
        "  tsne = TSNE(n_components=2)\n",
        "\n",
        "  try:\n",
        "    sne_proj = tsne.fit_transform(scores_df)\n",
        "    df_dimredu['inception_sne_1_scores'] = sne_proj[:,0]\n",
        "    df_dimredu['inception_sne_2_scores'] = sne_proj[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config scores')\n",
        "\n",
        "  try:  \n",
        "    sne_proj_data = tsne.fit_transform(data)\n",
        "    df_dataredu['inception_sne_1_data'] = sne_proj_data[:,0]\n",
        "    df_dataredu['inception_sne_2_data'] = sne_proj_data[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config data')\n",
        "\n",
        "  try:\n",
        "    sne_proj_bin = tsne.fit_transform(binary_scores_df)\n",
        "    df_binredu['inception_sne_1_binary'] = sne_proj_bin[:,0]\n",
        "    df_binredu['inception_sne_2_binary'] = sne_proj_bin[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config binary')\n",
        "\n",
        "  clf = LinearDiscriminantAnalysis()\n",
        "\n",
        "  try:\n",
        "    clf.fit(scores_df, target)\n",
        "    lda_proj = clf.transform(scores_df)\n",
        "    df_dimredu['inception_lda_1_scores_0'] = lda_proj[0:lda_proj.shape[0], 0]\n",
        "    df_dimredu['inception_lda_1_scores_1'] = lda_proj[0:lda_proj.shape[0], 1]\n",
        "    df_dimredu['inception_lda_1_scores_2'] = lda_proj[0:lda_proj.shape[0], 2]\n",
        "    df_dimredu['inception_lda_1_scores_3'] = lda_proj[0:lda_proj.shape[0], 3]\n",
        "    df_dimredu['inception_lda_1_scores_4'] = lda_proj[0:lda_proj.shape[0], 4]\n",
        "    df_dimredu['inception_lda_1_scores_5'] = lda_proj[0:lda_proj.shape[0], 5]\n",
        "    df_dimredu['inception_lda_1_scores_6'] = lda_proj[0:lda_proj.shape[0], 6]\n",
        "    df_dimredu['inception_lda_1_scores_7'] = lda_proj[0:lda_proj.shape[0], 7]\n",
        "    df_dimredu['inception_lda_1_scores_8'] = lda_proj[0:lda_proj.shape[0], 8]\n",
        "    df_dimredu['inception_lda_1_scores_9'] = lda_proj[0:lda_proj.shape[0], 9]\n",
        "    df_dimredu['inception_lda_1_scores_10'] = lda_proj[0:lda_proj.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis (classes exceeded)')\n",
        "\n",
        "  try:\n",
        "    clf.fit(data, target)\n",
        "    lda_proj_data = clf.transform(data)\n",
        "    df_dataredu['inception_lda_1_data_0'] = lda_proj_data[0:lda_proj_data.shape[0], 0]\n",
        "    df_dataredu['inception_lda_1_data_1'] = lda_proj_data[0:lda_proj_data.shape[0], 1]\n",
        "    df_dataredu['inception_lda_1_data_2'] = lda_proj_data[0:lda_proj_data.shape[0], 2]\n",
        "    df_dataredu['inception_lda_1_data_3'] = lda_proj_data[0:lda_proj_data.shape[0], 3]\n",
        "    df_dataredu['inception_lda_1_data_4'] = lda_proj_data[0:lda_proj_data.shape[0], 4]\n",
        "    df_dataredu['inception_lda_1_data_5'] = lda_proj_data[0:lda_proj_data.shape[0], 5]\n",
        "    df_dataredu['inception_lda_1_data_6'] = lda_proj_data[0:lda_proj_data.shape[0], 6]\n",
        "    df_dataredu['inception_lda_1_data_7'] = lda_proj_data[0:lda_proj_data.shape[0], 7]\n",
        "    df_dataredu['inception_lda_1_data_8'] = lda_proj_data[0:lda_proj_data.shape[0], 8]\n",
        "    df_dataredu['inception_lda_1_data_9'] = lda_proj_data[0:lda_proj_data.shape[0], 9]\n",
        "    df_dataredu['inception_lda_1_data_10'] = lda_proj_data[0:lda_proj_data.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)')\n",
        "\n",
        "  try:\n",
        "    clf.fit(binary_scores_df, target)\n",
        "    lda_proj_bin = clf.transform(binary_scores_df) \n",
        "    df_binredu['inception_lda_1_binary_0'] = lda_proj_bin[0:lda_proj_bin.shape[0], 0]\n",
        "    df_binredu['inception_lda_1_binary_1'] = lda_proj_bin[0:lda_proj_bin.shape[0], 1]\n",
        "    df_binredu['inception_lda_1_binary_2'] = lda_proj_bin[0:lda_proj_bin.shape[0], 2]\n",
        "    df_binredu['inception_lda_1_binary_3'] = lda_proj_bin[0:lda_proj_bin.shape[0], 3]\n",
        "    df_binredu['inception_lda_1_binary_4'] = lda_proj_bin[0:lda_proj_bin.shape[0], 4]\n",
        "    df_binredu['inception_lda_1_binary_5'] = lda_proj_bin[0:lda_proj_bin.shape[0], 5]\n",
        "    df_binredu['inception_lda_1_binary_6'] = lda_proj_bin[0:lda_proj_bin.shape[0], 6]\n",
        "    df_binredu['inception_lda_1_binary_7'] = lda_proj_bin[0:lda_proj_bin.shape[0], 7]\n",
        "    df_binredu['inception_lda_1_binary_8'] = lda_proj_bin[0:lda_proj_bin.shape[0], 8]\n",
        "    df_binredu['inception_lda_1_binary_9'] = lda_proj_bin[0:lda_proj_bin.shape[0], 9]\n",
        "    df_binredu['inception_lda_1_binary_10'] = lda_proj_bin[0:lda_proj_bin.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuzd5QUtD88d"
      },
      "source": [
        "#APPLY FEATURE EXTRACTION PIPELINE ON PROJECTED FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCqXbF11D88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9860ab33-5c15-45a6-df6b-4536634a7d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- --- --- --- ---\n",
            "Dimensionality Reduction Features Extraction: \n",
            "Exception Raised --> PCA Config 2\n",
            "Exception Raised --> PCA Config 3\n",
            "Exception Raised --> PCA Config loading\n",
            "Exception Raised --> Random Projection Config scores\n",
            "Exception Raised --> Random Projection Config binary\n",
            "Exception Raised --> TSNE Config scores\n",
            "Exception Raised --> TSNE Config binary\n",
            "Exception Raised --> Linear Discriminant Analysis (classes exceeded)\n",
            "Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)\n",
            "Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)\n",
            "Support Features Extraction Time: 82.59218 seconds\n",
            "\n",
            "Null Values in Dimensionality Reduction DataSets:\n",
            "inception_pca1_data         0\n",
            "inception_pca2_data         0\n",
            "inception_sub_proj1_data    0\n",
            "inception_sub_proj2_data    0\n",
            "inception_sne_1_data        0\n",
            "inception_sne_2_data        0\n",
            "inception_lda_1_data_0      0\n",
            "dtype: int64\n",
            "--- --- ---\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "#apply dimensionality reduction methods on principal components\n",
        "print('\\n--- --- --- --- ---\\nDimensionality Reduction Features Extraction: ')\n",
        "\n",
        "start_time = time.time()\n",
        "froid_light(data2)\n",
        "end = time.time()\n",
        "\n",
        "#save first feature extraction phase time ellapsed\n",
        "print(\"Support Features Extraction Time: %.8s seconds\" % (end - start_time))\n",
        "print('\\nNull Values in Dimensionality Reduction DataSets:')\n",
        "df_dataredu.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "print(df_dataredu.isnull().sum())\n",
        "print('--- --- ---')\n",
        "\n",
        "#normalize calculated principal components\n",
        "scaler = RobustScaler()\n",
        "df_dataredu = pd.DataFrame(scaler.fit_transform(df_dataredu), columns=df_dataredu.columns)\n",
        "\n",
        "#save dimensionality reduction (applied on principal components) to .csv\n",
        "df_dataredu.to_csv(\"inception_dim_redu_original_data_light.csv\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "c-1PW4MHDATY",
        "s-5r-qRIDATY",
        "_bOjTQ72DATZ",
        "W0h5MIoF35Zs"
      ],
      "name": "FROID_LITE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}