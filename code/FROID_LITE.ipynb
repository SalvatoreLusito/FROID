{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVbkG54Q9TEL"
      },
      "source": [
        "# IMPORT LIBRARIES AND FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sammon(x, n, display = 0, inputdist = 'raw', maxhalves = 20, maxiter = 1, tolfun = 1e-9, init = 'default'):\n",
        "\n",
        "    import numpy as np \n",
        "    from scipy.spatial.distance import cdist\n",
        "\n",
        "    \"\"\"Perform Sammon mapping on dataset x\n",
        "    y = sammon(x) applies the Sammon nonlinear mapping procedure on\n",
        "    multivariate data x, where each row represents a pattern and each column\n",
        "    represents a feature.  On completion, y contains the corresponding\n",
        "    co-ordinates of each point on the map.  By default, a two-dimensional\n",
        "    map is created.  Note if x contains any duplicated rows, SAMMON will\n",
        "    fail (ungracefully). \n",
        "    [y,E] = sammon(x) also returns the value of the cost function in E (i.e.\n",
        "    the ess of the mapping).\n",
        "    An N-dimensional output map is generated by y = sammon(x,n) .\n",
        "    A set of optimisation options can be specified using optional\n",
        "    arguments, y = sammon(x,n,[OPTS]):\n",
        "       maxiter        - maximum number of iterations\n",
        "       tolfun         - relative tolerance on objective function\n",
        "       maxhalves      - maximum number of step halvings\n",
        "       input          - {'raw','distance'} if set to 'distance', X is \n",
        "                        interpreted as a matrix of pairwise distances.\n",
        "       display        - 0 to 2. 0 least verbose, 2 max verbose.\n",
        "       init           - {'pca', 'cmdscale', random', 'default'}\n",
        "                        default is 'pca' if input is 'raw', \n",
        "                        'msdcale' if input is 'distance'\n",
        "    The default options are retrieved by calling sammon(x) with no\n",
        "    parameters.\n",
        "    File        : sammon.py\n",
        "    Date        : 18 April 2014\n",
        "    Authors     : Tom J. Pollard (tom.pollard.11@ucl.ac.uk)\n",
        "                : Ported from MATLAB implementation by \n",
        "                  Gavin C. Cawley and Nicola L. C. Talbot\n",
        "    Description : Simple python implementation of Sammon's non-linear\n",
        "                  mapping algorithm [1].\n",
        "    References  : [1] Sammon, John W. Jr., \"A Nonlinear Mapping for Data\n",
        "                  Structure Analysis\", IEEE Transactions on Computers,\n",
        "                  vol. C-18, no. 5, pp 401-409, May 1969.\n",
        "    Copyright   : (c) Dr Gavin C. Cawley, November 2007.\n",
        "    This program is free software; you can redistribute it and/or modify\n",
        "    it under the terms of the GNU General Public License as published by\n",
        "    the Free Software Foundation; either version 2 of the License, or\n",
        "    (at your option) any later version.\n",
        "    This program is distributed in the hope that it will be useful,\n",
        "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "    GNU General Public License for more details.\n",
        "    You should have received a copy of the GNU General Public License\n",
        "    along with this program; if not, write to the Free Software\n",
        "    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\n",
        "    \"\"\"\n",
        "\n",
        "    # Create distance matrix unless given by parameters\n",
        "    if inputdist == 'distance':\n",
        "        D = x\n",
        "        if init == 'default':\n",
        "            init = 'cmdscale'\n",
        "    else:\n",
        "        D = cdist(x, x)\n",
        "        if init == 'default':\n",
        "            init = 'pca'\n",
        "\n",
        "    if inputdist == 'distance' and init == 'pca':\n",
        "        raise ValueError(\"Cannot use init == 'pca' when inputdist == 'distance'\")\n",
        "\n",
        "    if np.count_nonzero(np.diagonal(D)) > 0:\n",
        "        raise ValueError(\"The diagonal of the dissimilarity matrix must be zero\")\n",
        "\n",
        "    # Remaining initialisation\n",
        "    N = x.shape[0]\n",
        "    scale = 0.5 / D.sum()\n",
        "    D = D + np.eye(N)     \n",
        "\n",
        "    if np.count_nonzero(D<=0) > 0:\n",
        "        raise ValueError(\"Off-diagonal dissimilarities must be strictly positive\")   \n",
        "\n",
        "    Dinv = 1 / D\n",
        "    if init == 'pca':\n",
        "        [UU,DD,_] = np.linalg.svd(x)\n",
        "        y = UU[:,:n]*DD[:n] \n",
        "    elif init == 'cmdscale':\n",
        "        from cmdscale import cmdscale\n",
        "        y,e = cmdscale(D)\n",
        "        y = y[:,:n]\n",
        "    else:\n",
        "        y = np.random.normal(0.0,1.0,[N,n])\n",
        "    one = np.ones([N,n])\n",
        "    d = cdist(y,y) + np.eye(N)\n",
        "    dinv = 1. / d\n",
        "    delta = D-d \n",
        "    E = ((delta**2)*Dinv).sum() \n",
        "\n",
        "    # Get on with it\n",
        "    for i in range(maxiter):\n",
        "        # Compute gradient, Hessian and search direction (note it is actually\n",
        "        # 1/4 of the gradient and Hessian, but the step size is just the ratio\n",
        "        # of the gradient and the diagonal of the Hessian so it doesn't\n",
        "        # matter).\n",
        "        delta = dinv - Dinv\n",
        "        deltaone = np.dot(delta,one)\n",
        "        g = np.dot(delta,y) - (y * deltaone)\n",
        "        dinv3 = dinv ** 3\n",
        "        y2 = y ** 2\n",
        "        H = np.dot(dinv3,y2) - deltaone - np.dot(2,y) * np.dot(dinv3,y) + y2 * np.dot(dinv3,one)\n",
        "        s = -g.flatten(order='F') / np.abs(H.flatten(order='F'))\n",
        "        y_old    = y\n",
        "\n",
        "        # Use step-halving procedure to ensure progress is made\n",
        "        for j in range(maxhalves):\n",
        "            s_reshape = np.reshape(s, (-1,n),order='F')\n",
        "            y = y_old + s_reshape\n",
        "            d = cdist(y, y) + np.eye(N)\n",
        "            dinv = 1 / d\n",
        "            delta = D - d\n",
        "            E_new = ((delta**2)*Dinv).sum()\n",
        "            if E_new < E:\n",
        "                break\n",
        "            else:\n",
        "                s = 0.5*s\n",
        "\n",
        "        # Bomb out if too many halving steps are required\n",
        "        if j == maxhalves-1:\n",
        "            print('Warning: maxhalves exceeded. Sammon mapping may not converge...')\n",
        "\n",
        "        # Evaluate termination criterion\n",
        "        if abs((E - E_new) / E) < tolfun:\n",
        "            if display:\n",
        "                print('TolFun exceeded: Optimisation terminated')\n",
        "            break\n",
        "\n",
        "        # Report progress\n",
        "        E = E_new\n",
        "        if display > 1:\n",
        "            print('epoch = %d : E = %12.10f'% (i+1, E * scale))\n",
        "\n",
        "    if i == maxiter-1:\n",
        "        print('Warning: maxiter exceeded. Sammon mapping may not have converged...')\n",
        "\n",
        "    # Fiddle stress to match the original Sammon paper\n",
        "    E = E * scale\n",
        "    \n",
        "    return [y,E]"
      ],
      "metadata": {
        "id": "DDQlmrPriOjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwwM5dIC8SCx"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import random \n",
        "import time\n",
        "from collections import defaultdict\n",
        "from scipy.stats.stats import pearsonr\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, roc_curve, auc, plot_confusion_matrix, roc_auc_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from PyNomaly import loop\n",
        "\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# evaluate model performance with outliers removed using isolation forest\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from pyod.models.knn import KNN   # kNN detector\n",
        "import pyod\n",
        "from pyod.utils.data import generate_data\n",
        "from pyod.utils.utility import score_to_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmSiOgLI5hlg"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlthKTNg9Xb9"
      },
      "source": [
        "# IMPORT DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_3HvJKu7_no"
      },
      "outputs": [],
      "source": [
        "dataname = 'cardio.csv'\n",
        "\n",
        "data = pd.read_csv(dataname, delimiter=',') #load data\n",
        "data.drop('Unnamed: 0', inplace=True, axis=1)\n",
        "#data.drop('outlier', inplace=True, axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT0SnETRnbpo"
      },
      "outputs": [],
      "source": [
        "target = data['class']\n",
        "data.drop('class', inplace=True, axis=1) \n",
        "\n",
        "#supervised?\n",
        "#data.drop('outlier', inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FROID-LIGHT"
      ],
      "metadata": {
        "id": "dZh47Q2AlCtv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uMYCNPq5sp0"
      },
      "outputs": [],
      "source": [
        "scores_df = pd.DataFrame()\n",
        "binary_scores_df = pd.DataFrame()\n",
        "df_dimredu = pd.DataFrame()\n",
        "df_dataredu = pd.DataFrame()\n",
        "df_binredu = pd.DataFrame()\n",
        "\n",
        "from pyod.models.mcd import MCD\n",
        "from pyod.models.suod import SUOD\n",
        "from pyod.models.loda import LODA\n",
        "from pyod.models.feature_bagging import FeatureBagging\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.cblof import CBLOF\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.cof import COF\n",
        "from pyod.models.copod import COPOD\n",
        "from pyod.models.ecod import ECOD\n",
        "from pyod.models.sos import SOS\n",
        "from pyod.models.pca import PCA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import random_projection\n",
        "from sklearn.manifold import Isomap\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras import regularizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import KernelPCA\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn import manifold\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.ticker import NullFormatter\n",
        "\n",
        "def froid_light(data):\n",
        "\n",
        "  try:\n",
        "    print('EllipticEnvelope')\n",
        "    ee = EllipticEnvelope()\n",
        "    score = ee.fit_predict(data)\n",
        "    dec_func = ee.decision_function(data)\n",
        "    maha = ee.mahalanobis(data)\n",
        "    pred = ee.predict(data)\n",
        "    scores_df['elliptic_decfunc_original_cont0.001'] = dec_func\n",
        "    scores_df['elliptic_mahalanobis_original_cont0.001'] = maha\n",
        "    binary_scores_df['elliptic_env_original_binary_cont0.001'] = pred\n",
        "    binary_scores_df['elliptic_env_original_binary_cont0.001'] = np.where(binary_scores_df['elliptic_env_original_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Elliptic Envelope Config 1')\n",
        "\n",
        "  try:\n",
        "    print('IsolationForest')\n",
        "    #isolation forest\n",
        "    iso = IsolationForest(n_jobs=-1)\n",
        "    #returns -1 for outliers and +1 for inliers\n",
        "    pred = iso.fit_predict(data)\n",
        "    #using decision function i can obtain outlierness for each observation\n",
        "    sklearn_score_anomalies = iso.decision_function(data)\n",
        "    original_paper_score = [-1*s + 0.5 for s in sklearn_score_anomalies]\n",
        "    scores_df['iso_forest_paper_score_original_cont0.001'] = original_paper_score\n",
        "    binary_scores_df['iso_forest_paper_score_original_binary_cont0.001'] = pred\n",
        "    binary_scores_df['iso_forest_paper_score_original_binary_cont0.001'] = np.where(binary_scores_df['iso_forest_paper_score_original_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Iso Forest Config 1')\n",
        "\n",
        "  try:\n",
        "    print('LOF')\n",
        "    #Local Outlier Factor\n",
        "    lof = LocalOutlierFactor(novelty=True, metric='minkowski',n_jobs=-1)\n",
        "    lof.fit(data)\n",
        "    lof_score = lof.decision_function(data)\n",
        "    pred = lof.predict(data)\n",
        "    scores_df['lof_score_original_mink_nei10'] = lof_score\n",
        "    binary_scores_df['lof_score_original_binary_mink_nei10'] = pred\n",
        "    binary_scores_df['lof_score_original_binary_mink_nei10'] = np.where(binary_scores_df['lof_score_original_binary_mink_nei10']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> LOF Config 1')\n",
        "\n",
        "  try:\n",
        "    print('COPOD')\n",
        "    clf = COPOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_copod = clf.decision_scores_  # raw outlier scores on the train data\n",
        "    scores_df['pyod_copod_original_cont0.001'] = pyod_copod\n",
        "    binary_scores_df['pyod_copod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> COPOD Config 1')\n",
        "\n",
        "  try:\n",
        "    print('ECOD')\n",
        "    clf = ECOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_ecod = clf.decision_scores_\n",
        "    scores_df['pyod_ecod_original_cont0.001'] = pyod_ecod\n",
        "    binary_scores_df['pyod_ecod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> ECOD Config 1')\n",
        "\n",
        "  try:\n",
        "    print('PCA')\n",
        "    clf = PCA(n_components=2)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_pca = clf.decision_scores_\n",
        "    scores_df['pyod_pca_original_cont0.001_whitFalse'] = pyod_pca\n",
        "    binary_scores_df['pyod_pca_original_binary_cont0.001_whitFalse'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> PCA Config 1 data maybe not converged')\n",
        "\n",
        "  try:\n",
        "    print('MCD')\n",
        "    clf = MCD()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_mcd = clf.decision_scores_\n",
        "    scores_df['pyod_mcd_original_cont0.001'] = pyod_mcd\n",
        "    binary_scores_df['pyod_mcd_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> MCD Config 1')\n",
        "\n",
        "  try:\n",
        "    print('COF')\n",
        "    clf = COF()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cof = clf.decision_scores_\n",
        "    scores_df['pyod_cof_original_cont0.001_nei10'] = pyod_cof\n",
        "    binary_scores_df['pyod_cof_original_binary_cont0.001_nei10'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> COF Config 1')\n",
        "\n",
        "  try:\n",
        "    print('CBLOF')\n",
        "    clf = CBLOF(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cblof = clf.decision_scores_\n",
        "    scores_df['pyod_cblof_original_cont0.001'] = pyod_cblof\n",
        "    binary_scores_df['pyod_cblof_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> CBLOF Config 1')\n",
        "\n",
        "  try:\n",
        "    print('HBOS')\n",
        "    clf = HBOS()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_hbos = clf.decision_scores_\n",
        "    scores_df['pyod_hbos_original_cont0.001'] = pyod_hbos\n",
        "    binary_scores_df['pyod_hbos_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> HBOS Config 1')\n",
        "\n",
        "  try:\n",
        "    print('KNN')\n",
        "    clf = KNN(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_knn = clf.decision_scores_\n",
        "    scores_df['pyod_knn_original_cont0.001'] = pyod_knn\n",
        "    binary_scores_df['pyod_knn_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> KNN Config 1')\n",
        "\n",
        "  try:\n",
        "    print('FeatureBagging')\n",
        "    clf = FeatureBagging(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_featbagg = clf.decision_scores_\n",
        "    scores_df['pyod_featbagg_original_cont0.001'] = pyod_featbagg\n",
        "    binary_scores_df['pyod_featbagg_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> Feature Bagging Config 1')\n",
        "\n",
        "  try:\n",
        "    print('LODA')\n",
        "    clf = LODA()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_loda = clf.decision_scores_\n",
        "    scores_df['pyod_loda_original_cont0.001'] = pyod_loda\n",
        "    binary_scores_df['pyod_loda_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> LODA Config 1')\n",
        "\n",
        "  try:\n",
        "    print('SUOD')\n",
        "    clf = SUOD()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_suod = clf.decision_scores_\n",
        "    scores_df['pyod_suod_original_cont0.001'] = pyod_suod\n",
        "    binary_scores_df['pyod_suod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> SUOD Config 1')\n",
        "\n",
        "  print('')\n",
        "  print('Dimensionality Reduction Algorithms: ')\n",
        "  print('PCA')\n",
        "  pca = PCA(n_components=2)\n",
        "  out_pca = pd.DataFrame()\n",
        "  out_pca_scores = pd.DataFrame()\n",
        "  out_pca_bin = pd.DataFrame()\n",
        "  try:\n",
        "    pca.fit(data)\n",
        "    out_pca = pd.DataFrame(pca.transform(data), index=data.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 1')\n",
        "\n",
        "  try:\n",
        "    pca.fit(scores_df)\n",
        "    out_pca_scores = pd.DataFrame(pca.transform(scores_df), index=scores_df.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 2')\n",
        "\n",
        "  try:\n",
        "    pca.fit(binary_scores_df)\n",
        "    out_pca_bin = pd.DataFrame(pca.transform(binary_scores_df), index=binary_scores_df.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 3')\n",
        "\n",
        "  try:\n",
        "    df_dataredu['pca1_data'] = out_pca[0]\n",
        "    df_dataredu['pca2_data'] = out_pca[1]\n",
        "    df_dimredu['pca1_scores'] = out_pca_scores[0]\n",
        "    df_dimredu['pca2_scores'] = out_pca_scores[1]\n",
        "    df_binredu['pca1_binary'] = out_pca_bin[0]\n",
        "    df_binredu['pca2_binary'] = out_pca_bin[1]\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config loading')\n",
        "\n",
        "  print('RandomGaussianProjection')\n",
        "  rsp = random_projection.GaussianRandomProjection(n_components=2)\n",
        "  try:\n",
        "    ran_proj = rsp.fit_transform(scores_df)\n",
        "    df_dimredu['sub_proj1_scores'] = ran_proj[:,0]\n",
        "    df_dimredu['sub_proj2_scores'] = ran_proj[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config scores')\n",
        "\n",
        "  try:  \n",
        "    ran_proj_data = rsp.fit_transform(data)\n",
        "    df_dataredu['sub_proj1_data'] = ran_proj_data[:,0]\n",
        "    df_dataredu['sub_proj2_data'] = ran_proj_data[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config data')\n",
        "\n",
        "  try:\n",
        "    ran_proj_bin = rsp.fit_transform(binary_scores_df)\n",
        "    df_binredu['sub_proj1_binary'] = ran_proj_bin[:,0]\n",
        "    df_binredu['sub_proj2_binary'] = ran_proj_bin[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config binary')\n",
        "\n",
        "  print('TSNE')\n",
        "  tsne = TSNE(n_components=2)\n",
        "\n",
        "  try:\n",
        "    sne_proj = tsne.fit_transform(scores_df)\n",
        "    df_dimredu['sne_1_scores'] = sne_proj[:,0]\n",
        "    df_dimredu['sne_2_scores'] = sne_proj[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config scores')\n",
        "\n",
        "  try:  \n",
        "    sne_proj_data = tsne.fit_transform(data)\n",
        "    df_dataredu['sne_1_data'] = sne_proj_data[:,0]\n",
        "    df_dataredu['sne_2_data'] = sne_proj_data[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config data')\n",
        "\n",
        "  try:\n",
        "    sne_proj_bin = tsne.fit_transform(binary_scores_df)\n",
        "    df_binredu['sne_1_binary'] = sne_proj_bin[:,0]\n",
        "    df_binredu['sne_2_binary'] = sne_proj_bin[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config binary')\n",
        "\n",
        "  print('LinearDiscriminantAnalysis')\n",
        "  clf = LinearDiscriminantAnalysis()\n",
        "\n",
        "  try:\n",
        "    clf.fit(scores_df, target)\n",
        "    lda_proj = clf.transform(scores_df)\n",
        "    df_dimredu['lda_1_scores_0'] = lda_proj[0:lda_proj.shape[0], 0]\n",
        "    df_dimredu['lda_1_scores_1'] = lda_proj[0:lda_proj.shape[0], 1]\n",
        "    df_dimredu['lda_1_scores_2'] = lda_proj[0:lda_proj.shape[0], 2]\n",
        "    df_dimredu['lda_1_scores_3'] = lda_proj[0:lda_proj.shape[0], 3]\n",
        "    df_dimredu['lda_1_scores_4'] = lda_proj[0:lda_proj.shape[0], 4]\n",
        "    df_dimredu['lda_1_scores_5'] = lda_proj[0:lda_proj.shape[0], 5]\n",
        "    df_dimredu['lda_1_scores_6'] = lda_proj[0:lda_proj.shape[0], 6]\n",
        "    df_dimredu['lda_1_scores_7'] = lda_proj[0:lda_proj.shape[0], 7]\n",
        "    df_dimredu['lda_1_scores_8'] = lda_proj[0:lda_proj.shape[0], 8]\n",
        "    df_dimredu['lda_1_scores_9'] = lda_proj[0:lda_proj.shape[0], 9]\n",
        "    df_dimredu['lda_1_scores_10'] = lda_proj[0:lda_proj.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis (classes exceeded)')\n",
        "\n",
        "  try:\n",
        "    clf.fit(data, target)\n",
        "    lda_proj_data = clf.transform(data)\n",
        "    df_dataredu['lda_1_data_0'] = lda_proj_data[0:lda_proj_data.shape[0], 0]\n",
        "    df_dataredu['lda_1_data_1'] = lda_proj_data[0:lda_proj_data.shape[0], 1]\n",
        "    df_dataredu['lda_1_data_2'] = lda_proj_data[0:lda_proj_data.shape[0], 2]\n",
        "    df_dataredu['lda_1_data_3'] = lda_proj_data[0:lda_proj_data.shape[0], 3]\n",
        "    df_dataredu['lda_1_data_4'] = lda_proj_data[0:lda_proj_data.shape[0], 4]\n",
        "    df_dataredu['lda_1_data_5'] = lda_proj_data[0:lda_proj_data.shape[0], 5]\n",
        "    df_dataredu['lda_1_data_6'] = lda_proj_data[0:lda_proj_data.shape[0], 6]\n",
        "    df_dataredu['lda_1_data_7'] = lda_proj_data[0:lda_proj_data.shape[0], 7]\n",
        "    df_dataredu['lda_1_data_8'] = lda_proj_data[0:lda_proj_data.shape[0], 8]\n",
        "    df_dataredu['lda_1_data_9'] = lda_proj_data[0:lda_proj_data.shape[0], 9]\n",
        "    df_dataredu['lda_1_data_10'] = lda_proj_data[0:lda_proj_data.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)')\n",
        "\n",
        "  try:\n",
        "    clf.fit(binary_scores_df, target)\n",
        "    lda_proj_bin = clf.transform(binary_scores_df) \n",
        "    df_binredu['lda_1_binary_0'] = lda_proj_bin[0:lda_proj_bin.shape[0], 0]\n",
        "    df_binredu['lda_1_binary_1'] = lda_proj_bin[0:lda_proj_bin.shape[0], 1]\n",
        "    df_binredu['lda_1_binary_2'] = lda_proj_bin[0:lda_proj_bin.shape[0], 2]\n",
        "    df_binredu['lda_1_binary_3'] = lda_proj_bin[0:lda_proj_bin.shape[0], 3]\n",
        "    df_binredu['lda_1_binary_4'] = lda_proj_bin[0:lda_proj_bin.shape[0], 4]\n",
        "    df_binredu['lda_1_binary_5'] = lda_proj_bin[0:lda_proj_bin.shape[0], 5]\n",
        "    df_binredu['lda_1_binary_6'] = lda_proj_bin[0:lda_proj_bin.shape[0], 6]\n",
        "    df_binredu['lda_1_binary_7'] = lda_proj_bin[0:lda_proj_bin.shape[0], 7]\n",
        "    df_binredu['lda_1_binary_8'] = lda_proj_bin[0:lda_proj_bin.shape[0], 8]\n",
        "    df_binredu['lda_1_binary_9'] = lda_proj_bin[0:lda_proj_bin.shape[0], 9]\n",
        "    df_binredu['lda_1_binary_10'] = lda_proj_bin[0:lda_proj_bin.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H9dBtOaOpyP"
      },
      "source": [
        "#APPLY FEATURE EXTRACTION PIPELINE ON ORIGINAL DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1UTkoUYKOi3"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print('Feature Extraction: \\n')\n",
        "froid_light(data)\n",
        "\n",
        "end = time.time()\n",
        "#save first feature extraction phase time ellapsed\n",
        "print(\"Support Features Extraction Time: %.8s seconds\" % (end - start_time))\n",
        "\n",
        "# --- --- --- --- --- #\n",
        "\n",
        "print('\\nNull Values in Outlierness Scores DataSets:')\n",
        "print('Scores_df: ')\n",
        "scores_df.loc[:, scores_df.isna().any()]\n",
        "scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "print('--- --- ---')\n",
        "print('Binary_Scores_df: ')\n",
        "binary_scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "binary_scores_df.loc[:, binary_scores_df.isna().any()]\n",
        "print('--- --- ---')\n",
        "print('\\nNull Values in Dimensionality Reduction DataSets:')\n",
        "df_dimredu.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "print(df_dimredu.isnull().sum())\n",
        "print('--- --- ---')\n",
        "df_dataredu.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "print(df_dataredu.isnull().sum())\n",
        "print('--- --- ---')\n",
        "df_binredu.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "print(df_binredu.isnull().sum())\n",
        "print('--- --- ---')\n",
        "\n",
        "# --- --- --- --- --- #\n",
        "\n",
        "#normalize outlierness scores\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "scores_df = pd.DataFrame(scaler.fit_transform(scores_df), columns=scores_df.columns)\n",
        "scores_df.to_csv( \"outlierness_scores_light.csv\")\n",
        "binary_scores_df.to_csv(\"binary_scores_light.csv\")\n",
        "\n",
        "df_dimredu = pd.DataFrame(scaler.fit_transform(df_dimredu), columns=df_dimredu.columns)\n",
        "df_dataredu = pd.DataFrame(scaler.fit_transform(df_dataredu), columns=df_dataredu.columns)\n",
        "df_binredu = pd.DataFrame(scaler.fit_transform(df_binredu), columns=df_binredu.columns)\n",
        "df_dimredu.to_csv( \"dim_redu_scores_light.csv\")\n",
        "df_dataredu.to_csv(\"dim_redu_original_data_light.csv\")\n",
        "df_binredu.to_csv(\"dim_redu_binary_scores_light.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-1PW4MHDATY"
      },
      "source": [
        "#OUTLIERNESS SCORES EXTRACTION ON PRINCIPAL COMPONENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJJZzXZiDTJg"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('dim_redu_original_data_light.csv', delimiter=',') #load data\n",
        "data.drop('Unnamed: 0', inplace=True, axis=1)\n",
        "\n",
        "original = pd.read_csv(dataname, delimiter=',') #load data\n",
        "original.drop('Unnamed: 0', inplace=True, axis=1)\n",
        "target = original['class']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-5r-qRIDATY"
      },
      "source": [
        "##pynomaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfi_KgeL0x1n"
      },
      "outputs": [],
      "source": [
        "scores_df = pd.DataFrame()\n",
        "binary_scores_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bOjTQ72DATZ"
      },
      "source": [
        "##sklearn algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4ZpafaA1IPm"
      },
      "outputs": [],
      "source": [
        "from pyod.models.copod import COPOD\n",
        "from pyod.models.suod import SUOD\n",
        "from pyod.models.loda import LODA\n",
        "from pyod.models.feature_bagging import FeatureBagging\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.cblof import CBLOF\n",
        "from pyod.models.cof import COF\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.mcd import MCD\n",
        "from pyod.models.pca import PCA\n",
        "from pyod.models.ecod import ECOD\n",
        "from pyod.models.sos import SOS\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "def froid_light(data):\n",
        "  try:\n",
        "    #Minimum Covariance Determinant\n",
        "    ee = EllipticEnvelope()\n",
        "    score = ee.fit_predict(data)\n",
        "    dec_func = ee.decision_function(data)\n",
        "    maha = ee.mahalanobis(data)\n",
        "    pred = ee.predict(data)\n",
        "    scores_df['elliptic_decfunc_dim_redu_cont0.001'] = dec_func\n",
        "    scores_df['elliptic_mahalanobis_dim_redu_cont0.001'] = maha\n",
        "    binary_scores_df['elliptic_env_dim_redu_binary_cont0.001'] = pred\n",
        "    binary_scores_df['elliptic_env_dim_redu_binary_cont0.001'] = np.where(binary_scores_df['elliptic_env_dim_redu_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Elliptic Envelope Config 1')\n",
        "\n",
        "  try:\n",
        "    #isolation forest\n",
        "    iso = IsolationForest(n_jobs=-1)\n",
        "    #returns -1 for outliers and +1 for inliers\n",
        "    pred = iso.fit_predict(data)\n",
        "    #using decision function i can obtain outlierness for each observation\n",
        "    sklearn_score_anomalies = iso.decision_function(data)\n",
        "    original_paper_score = [-1*s + 0.5 for s in sklearn_score_anomalies]\n",
        "    scores_df['iso_forest_paper_score_dim_redu_cont0.001'] = original_paper_score\n",
        "    binary_scores_df['iso_forest_paper_score_dim_redu_binary_cont0.001'] = pred\n",
        "    binary_scores_df['iso_forest_paper_score_dim_redu_binary_cont0.001'] = np.where(binary_scores_df['iso_forest_paper_score_dim_redu_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Isolation Forest Config 1')\n",
        "\n",
        "  try:\n",
        "    #Local Outlier Factor\n",
        "    lof = LocalOutlierFactor(novelty=True, metric='minkowski',n_jobs=-1)\n",
        "    lof.fit(data)\n",
        "    lof_score = lof.decision_function(data)\n",
        "    pred = lof.predict(data)\n",
        "    scores_df['lof_score_dim_redu_mink_nei10'] = lof_score\n",
        "    binary_scores_df['lof_score_dim_redu_binary_mink_nei10'] = pred\n",
        "    binary_scores_df['lof_score_dim_redu_binary_mink_nei10'] = np.where(binary_scores_df['lof_score_dim_redu_binary_mink_nei10']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> LOF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = COPOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_copod = clf.decision_scores_  # raw outlier scores on the train data\n",
        "    scores_df['pyod_copod_dim_redu_cont0.001'] = pyod_copod\n",
        "    binary_scores_df['pyod_copod_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> COPOD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = ECOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_ecod = clf.decision_scores_\n",
        "    scores_df['pyod_ecod_dim_redu_cont0.001'] = pyod_ecod\n",
        "    binary_scores_df['pyod_ecod_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> ECOD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = PCA(n_components=2,whiten=False)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_pca = clf.decision_scores_\n",
        "    scores_df['pyod_pca_dim_redu_cont0.001_whitFalse'] = pyod_pca\n",
        "    binary_scores_df['pyod_pca_dim_redu_binary_cont0.001_whitFalse'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = MCD()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_mcd = clf.decision_scores_\n",
        "    scores_df['pyod_mcd_dim_redu_cont0.001'] = pyod_mcd\n",
        "    binary_scores_df['pyod_mcd_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> MCD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = COF()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cof = clf.decision_scores_\n",
        "    scores_df['pyod_cof_dim_redu_cont0.001_nei10'] = pyod_cof\n",
        "    binary_scores_df['pyod_cof_dim_redu_binary_cont0.001_nei10'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> COF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = CBLOF(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cblof = clf.decision_scores_\n",
        "    scores_df['pyod_cblof_dim_redu_cont0.001'] = pyod_cblof\n",
        "    binary_scores_df['pyod_cblof_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> CBLOF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = HBOS()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_hbos = clf.decision_scores_\n",
        "    scores_df['pyod_hbos_dim_redu_cont0.001'] = pyod_hbos\n",
        "    binary_scores_df['pyod_hbos_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> HBOS Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = KNN(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_knn = clf.decision_scores_\n",
        "    scores_df['pyod_knn_dim_redu_cont0.001'] = pyod_knn\n",
        "    binary_scores_df['pyod_knn_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> KNN Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = FeatureBagging(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_featbagg = clf.decision_scores_\n",
        "    scores_df['pyod_featbagg_dim_redu_cont0.001'] = pyod_featbagg\n",
        "    binary_scores_df['pyod_featbagg_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> Feature Bagging Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = LODA()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_loda = clf.decision_scores_\n",
        "    scores_df['pyod_loda_dim_redu_cont0.001'] = pyod_loda\n",
        "    binary_scores_df['pyod_loda_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> LODA Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = SUOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_suod = clf.decision_scores_\n",
        "    scores_df['pyod_suod_dim_redu_cont0.001'] = pyod_suod\n",
        "    binary_scores_df['pyod_suod_dim_redu_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised --> SUOD Config 1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0h5MIoF35Zs"
      },
      "source": [
        "#APPLY FEATURE EXTRACTION PIPELINE ON PRINCIPAL COMPONENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tffutDv4BN1"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "import time\n",
        "\n",
        "print('Outlierness Scores from Principal Components Datasets: ')\n",
        "start_time = time.time()\n",
        "froid_light(data)\n",
        "end = time.time()\n",
        "\n",
        "#save first feature extraction phase time ellapsed\n",
        "print(\"Support Features on Principal Components Extraction Time: %.8s seconds\" % (end - start_time))\n",
        "print('\\nNull Values in Outlierness Scores DataSets:')\n",
        "print('Scores_df: ')\n",
        "scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "scores_df.loc[:, scores_df.isna().any()]\n",
        "print('--- --- ---')\n",
        "print('Binary_Scores_df: ')\n",
        "binary_scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "binary_scores_df.loc[:, binary_scores_df.isna().any()]\n",
        "print('--- --- ---')\n",
        "\n",
        "#normalize outlierness scores\n",
        "scaler = RobustScaler()\n",
        "scores_df = pd.DataFrame(scaler.fit_transform(scores_df), columns=scores_df.columns)\n",
        "\n",
        "scores_df.to_csv( \"scores_principal_comp_light.csv\")\n",
        "binary_scores_df.to_csv( \"binary_scores_principal_comp_light.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKjSFOGnDvyt"
      },
      "source": [
        "# INCEPTION FROID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrInPSL6DoGA"
      },
      "outputs": [],
      "source": [
        "dataname = 'outlierness_scores_light.csv'\n",
        "dataname2 = 'dim_redu_original_data_light.csv'\n",
        "\n",
        "data = pd.read_csv(dataname, delimiter=',') #load outlierness scores on original data -> apply anomaly detection on it\n",
        "data.drop('Unnamed: 0', inplace=True, axis=1)\n",
        "\n",
        "data2 = pd.read_csv(dataname2, delimiter=',') #load principal components on original data -> apply dimensionality reduction on it\n",
        "data2.drop('Unnamed: 0', inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvkxWvmiDvyt"
      },
      "outputs": [],
      "source": [
        "scores_df = pd.DataFrame()\n",
        "binary_scores_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCF1tx7TDvyz"
      },
      "outputs": [],
      "source": [
        "from pyod.models.mcd import MCD\n",
        "from pyod.models.suod import SUOD\n",
        "from pyod.models.loda import LODA\n",
        "from pyod.models.feature_bagging import FeatureBagging\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.models.cblof import CBLOF\n",
        "from pyod.models.ocsvm import OCSVM\n",
        "from pyod.models.cof import COF\n",
        "from pyod.models.copod import COPOD\n",
        "from pyod.models.ecod import ECOD\n",
        "from pyod.models.sos import SOS\n",
        "from pyod.models.pca import PCA\n",
        "\n",
        "def froid_light(data):\n",
        "  #Minimum Covariance Determinant\n",
        "  try:\n",
        "    ee = EllipticEnvelope()\n",
        "    score = ee.fit_predict(data)\n",
        "    dec_func = ee.decision_function(data)\n",
        "    maha = ee.mahalanobis(data)\n",
        "    pred = ee.predict(data)\n",
        "    scores_df['inception_elliptic_decfunc_original_cont0.001'] = dec_func\n",
        "    scores_df['inception_elliptic_mahalanobis_original_cont0.001'] = maha\n",
        "    binary_scores_df['inception_elliptic_env_original_binary_cont0.001'] = pred\n",
        "    binary_scores_df['inception_elliptic_env_original_binary_cont0.001'] = np.where(binary_scores_df['inception_elliptic_env_original_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Elliptic Envelope Config 1')\n",
        "\n",
        "  try:\n",
        "    #isolation forest\n",
        "    iso = IsolationForest(n_jobs=-1)\n",
        "    #returns -1 for outliers and +1 for inliers\n",
        "    pred = iso.fit_predict(data)\n",
        "    #using decision function i can obtain outlierness for each observation\n",
        "    sklearn_score_anomalies = iso.decision_function(data)\n",
        "    original_paper_score = [-1*s + 0.5 for s in sklearn_score_anomalies]\n",
        "    scores_df['inception_iso_forest_paper_score_original_cont0.001'] = original_paper_score\n",
        "    binary_scores_df['inception_iso_forest_paper_score_original_binary_cont0.001'] = pred\n",
        "    binary_scores_df['inception_iso_forest_paper_score_original_binary_cont0.001'] = np.where(binary_scores_df['inception_iso_forest_paper_score_original_binary_cont0.001']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> Iso Forest Config 1')\n",
        "\n",
        "  try:\n",
        "    #Local Outlier Factor\n",
        "    lof = LocalOutlierFactor(novelty=True, metric='minkowski',n_jobs=-1)\n",
        "    lof.fit(data)\n",
        "    lof_score = lof.decision_function(data)\n",
        "    pred = lof.predict(data)\n",
        "    scores_df['inception_lof_score_original_mink_nei10'] = lof_score\n",
        "    binary_scores_df['inception_lof_score_original_binary_mink_nei10'] = pred\n",
        "    binary_scores_df['inception_lof_score_original_binary_mink_nei10'] = np.where(binary_scores_df['inception_lof_score_original_binary_mink_nei10']==-1, 1,0)\n",
        "  except:\n",
        "    print('Exception Raised --> LOF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = COPOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_copod = clf.decision_scores_  # raw outlier scores on the train data\n",
        "    scores_df['inception_pyod_copod_original_cont0.001'] = pyod_copod\n",
        "    binary_scores_df['inception_pyod_copod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> COPOD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = ECOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_ecod = clf.decision_scores_\n",
        "    scores_df['inception_pyod_ecod_original_cont0.001'] = pyod_ecod\n",
        "    binary_scores_df['inception_pyod_ecod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> ECOD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = PCA(n_components=2)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_pca = clf.decision_scores_\n",
        "    scores_df['inception_pyod_pca_original_cont0.001_whitFalse'] = pyod_pca\n",
        "    binary_scores_df['inception_pyod_pca_original_binary_cont0.001_whitFalse'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> PCA Config 1 data maybe not converged')\n",
        "\n",
        "  try:\n",
        "    clf = MCD()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_mcd = clf.decision_scores_\n",
        "    scores_df['inception_pyod_mcd_original_cont0.001'] = pyod_mcd\n",
        "    binary_scores_df['inception_pyod_mcd_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> MCD Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = COF()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cof = clf.decision_scores_\n",
        "    scores_df['inception_pyod_cof_original_cont0.001_nei10'] = pyod_cof\n",
        "    binary_scores_df['inception_pyod_cof_original_binary_cont0.001_nei10'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> COF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = CBLOF(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_cblof = clf.decision_scores_\n",
        "    scores_df['inception_pyod_cblof_original_cont0.001'] = pyod_cblof\n",
        "    binary_scores_df['inception_pyod_cblof_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> CBLOF Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = HBOS()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_hbos = clf.decision_scores_\n",
        "    scores_df['inception_pyod_hbos_original_cont0.001'] = pyod_hbos\n",
        "    binary_scores_df['inception_pyod_hbos_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> HBOS Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = KNN(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_knn = clf.decision_scores_\n",
        "    scores_df['inception_pyod_knn_original_cont0.001'] = pyod_knn\n",
        "    binary_scores_df['inception_pyod_knn_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> KNN Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = FeatureBagging(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_featbagg = clf.decision_scores_\n",
        "    scores_df['inception_pyod_featbagg_original_cont0.001'] = pyod_featbagg\n",
        "    binary_scores_df['inception_pyod_featbagg_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> Feature Bagging Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = LODA()\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_loda = clf.decision_scores_\n",
        "    scores_df['inception_pyod_loda_original_cont0.001'] = pyod_loda\n",
        "    binary_scores_df['inception_pyod_loda_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> LODA Config 1')\n",
        "\n",
        "  try:\n",
        "    clf = SUOD(n_jobs=-1)\n",
        "    clf.fit(data)\n",
        "    pred = clf.predict(data)\n",
        "    pyod_suod = clf.decision_scores_\n",
        "    scores_df['inception_pyod_suod_original_cont0.001'] = pyod_suod\n",
        "    binary_scores_df['inception_pyod_suod_original_binary_cont0.001'] = pred\n",
        "  except:\n",
        "    print('Exception Raised -> SUOD Config 1')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df"
      ],
      "metadata": {
        "id": "VwMoaw2L2069"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print('Outlierness Scores Extraction: \\n')\n",
        "froid_light(data) #anomaly detection methods applied on outlierness scores\n",
        "\n",
        "#normalize outlierness scores\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "scores_df = pd.DataFrame(scaler.fit_transform(scores_df), columns=scores_df.columns)\n",
        "\n",
        "print('\\nNull Values in Outlierness Scores DataSets:')\n",
        "print('Scores_df: ')\n",
        "scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "scores_df.loc[:, scores_df.isna().any()]\n",
        "print('--- --- ---')\n",
        "print('Binary_Scores_df: ')\n",
        "binary_scores_df.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "binary_scores_df.loc[:, binary_scores_df.isna().any()]\n",
        "print('--- --- ---')\n",
        "\n",
        "#save outlierness numeric and binary scores to .csv\n",
        "scores_df.to_csv( \"inception_outlierness_scores_light.csv\")\n",
        "binary_scores_df.to_csv( \"inception_binary_scores_light.csv\")"
      ],
      "metadata": {
        "id": "-0ye8qQNdW0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw04x-ufD1vU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import random_projection\n",
        "from sklearn.manifold import Isomap\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras import regularizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import KernelPCA\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn import manifold\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.ticker import NullFormatter\n",
        "\n",
        "df_dimredu = pd.DataFrame()\n",
        "df_dataredu = pd.DataFrame()\n",
        "df_binredu = pd.DataFrame()\n",
        "\n",
        "def froid_light(data):\n",
        "  pca = PCA(n_components=2)\n",
        "  out_pca = pd.DataFrame()\n",
        "  out_pca_scores = pd.DataFrame()\n",
        "  out_pca_bin = pd.DataFrame()\n",
        "  try:\n",
        "    pca.fit(data)\n",
        "    out_pca = pd.DataFrame(pca.transform(data), index=data.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 1')\n",
        "\n",
        "  try:\n",
        "    pca.fit(scores_df)\n",
        "    out_pca_scores = pd.DataFrame(pca.transform(scores_df), index=scores_df.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 2')\n",
        "\n",
        "  try:\n",
        "    pca.fit(binary_scores_df)\n",
        "    out_pca_bin = pd.DataFrame(pca.transform(binary_scores_df), index=binary_scores_df.index)\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config 3')\n",
        "\n",
        "  try:\n",
        "    df_dataredu['inception_pca1_data'] = out_pca[0]\n",
        "    df_dataredu['inception_pca2_data'] = out_pca[1]\n",
        "    df_dimredu['inception_pca1_scores'] = out_pca_scores[0]\n",
        "    df_dimredu['inception_pca2_scores'] = out_pca_scores[1]\n",
        "    df_binredu['inception_pca1_binary'] = out_pca_bin[0]\n",
        "    df_binredu['inception_pca2_binary'] = out_pca_bin[1]\n",
        "  except:\n",
        "    print('Exception Raised --> PCA Config loading')\n",
        "\n",
        "  rsp = random_projection.GaussianRandomProjection(n_components=2)\n",
        "  try:\n",
        "    ran_proj = rsp.fit_transform(scores_df)\n",
        "    df_dimredu['inception_sub_proj1_scores'] = ran_proj[:,0]\n",
        "    df_dimredu['inception_sub_proj2_scores'] = ran_proj[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config scores')\n",
        "\n",
        "  try:  \n",
        "    ran_proj_data = rsp.fit_transform(data)\n",
        "    df_dataredu['inception_sub_proj1_data'] = ran_proj_data[:,0]\n",
        "    df_dataredu['inception_sub_proj2_data'] = ran_proj_data[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config data')\n",
        "\n",
        "  try:\n",
        "    ran_proj_bin = rsp.fit_transform(binary_scores_df)\n",
        "    df_binredu['inception_sub_proj1_binary'] = ran_proj_bin[:,0]\n",
        "    df_binredu['inception_sub_proj2_binary'] = ran_proj_bin[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> Random Projection Config binary')\n",
        "\n",
        "  tsne = TSNE(n_components=2)\n",
        "\n",
        "  try:\n",
        "    sne_proj = tsne.fit_transform(scores_df)\n",
        "    df_dimredu['inception_sne_1_scores'] = sne_proj[:,0]\n",
        "    df_dimredu['inception_sne_2_scores'] = sne_proj[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config scores')\n",
        "\n",
        "  try:  \n",
        "    sne_proj_data = tsne.fit_transform(data)\n",
        "    df_dataredu['inception_sne_1_data'] = sne_proj_data[:,0]\n",
        "    df_dataredu['inception_sne_2_data'] = sne_proj_data[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config data')\n",
        "\n",
        "  try:\n",
        "    sne_proj_bin = tsne.fit_transform(binary_scores_df)\n",
        "    df_binredu['inception_sne_1_binary'] = sne_proj_bin[:,0]\n",
        "    df_binredu['inception_sne_2_binary'] = sne_proj_bin[:,1]\n",
        "  except:\n",
        "    print('Exception Raised --> TSNE Config binary')\n",
        "\n",
        "  clf = LinearDiscriminantAnalysis()\n",
        "\n",
        "  try:\n",
        "    clf.fit(scores_df, target)\n",
        "    lda_proj = clf.transform(scores_df)\n",
        "    df_dimredu['inception_lda_1_scores_0'] = lda_proj[0:lda_proj.shape[0], 0]\n",
        "    df_dimredu['inception_lda_1_scores_1'] = lda_proj[0:lda_proj.shape[0], 1]\n",
        "    df_dimredu['inception_lda_1_scores_2'] = lda_proj[0:lda_proj.shape[0], 2]\n",
        "    df_dimredu['inception_lda_1_scores_3'] = lda_proj[0:lda_proj.shape[0], 3]\n",
        "    df_dimredu['inception_lda_1_scores_4'] = lda_proj[0:lda_proj.shape[0], 4]\n",
        "    df_dimredu['inception_lda_1_scores_5'] = lda_proj[0:lda_proj.shape[0], 5]\n",
        "    df_dimredu['inception_lda_1_scores_6'] = lda_proj[0:lda_proj.shape[0], 6]\n",
        "    df_dimredu['inception_lda_1_scores_7'] = lda_proj[0:lda_proj.shape[0], 7]\n",
        "    df_dimredu['inception_lda_1_scores_8'] = lda_proj[0:lda_proj.shape[0], 8]\n",
        "    df_dimredu['inception_lda_1_scores_9'] = lda_proj[0:lda_proj.shape[0], 9]\n",
        "    df_dimredu['inception_lda_1_scores_10'] = lda_proj[0:lda_proj.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis (classes exceeded)')\n",
        "\n",
        "  try:\n",
        "    clf.fit(data, target)\n",
        "    lda_proj_data = clf.transform(data)\n",
        "    df_dataredu['inception_lda_1_data_0'] = lda_proj_data[0:lda_proj_data.shape[0], 0]\n",
        "    df_dataredu['inception_lda_1_data_1'] = lda_proj_data[0:lda_proj_data.shape[0], 1]\n",
        "    df_dataredu['inception_lda_1_data_2'] = lda_proj_data[0:lda_proj_data.shape[0], 2]\n",
        "    df_dataredu['inception_lda_1_data_3'] = lda_proj_data[0:lda_proj_data.shape[0], 3]\n",
        "    df_dataredu['inception_lda_1_data_4'] = lda_proj_data[0:lda_proj_data.shape[0], 4]\n",
        "    df_dataredu['inception_lda_1_data_5'] = lda_proj_data[0:lda_proj_data.shape[0], 5]\n",
        "    df_dataredu['inception_lda_1_data_6'] = lda_proj_data[0:lda_proj_data.shape[0], 6]\n",
        "    df_dataredu['inception_lda_1_data_7'] = lda_proj_data[0:lda_proj_data.shape[0], 7]\n",
        "    df_dataredu['inception_lda_1_data_8'] = lda_proj_data[0:lda_proj_data.shape[0], 8]\n",
        "    df_dataredu['inception_lda_1_data_9'] = lda_proj_data[0:lda_proj_data.shape[0], 9]\n",
        "    df_dataredu['inception_lda_1_data_10'] = lda_proj_data[0:lda_proj_data.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)')\n",
        "\n",
        "  try:\n",
        "    clf.fit(binary_scores_df, target)\n",
        "    lda_proj_bin = clf.transform(binary_scores_df) \n",
        "    df_binredu['inception_lda_1_binary_0'] = lda_proj_bin[0:lda_proj_bin.shape[0], 0]\n",
        "    df_binredu['inception_lda_1_binary_1'] = lda_proj_bin[0:lda_proj_bin.shape[0], 1]\n",
        "    df_binredu['inception_lda_1_binary_2'] = lda_proj_bin[0:lda_proj_bin.shape[0], 2]\n",
        "    df_binredu['inception_lda_1_binary_3'] = lda_proj_bin[0:lda_proj_bin.shape[0], 3]\n",
        "    df_binredu['inception_lda_1_binary_4'] = lda_proj_bin[0:lda_proj_bin.shape[0], 4]\n",
        "    df_binredu['inception_lda_1_binary_5'] = lda_proj_bin[0:lda_proj_bin.shape[0], 5]\n",
        "    df_binredu['inception_lda_1_binary_6'] = lda_proj_bin[0:lda_proj_bin.shape[0], 6]\n",
        "    df_binredu['inception_lda_1_binary_7'] = lda_proj_bin[0:lda_proj_bin.shape[0], 7]\n",
        "    df_binredu['inception_lda_1_binary_8'] = lda_proj_bin[0:lda_proj_bin.shape[0], 8]\n",
        "    df_binredu['inception_lda_1_binary_9'] = lda_proj_bin[0:lda_proj_bin.shape[0], 9]\n",
        "    df_binredu['inception_lda_1_binary_10'] = lda_proj_bin[0:lda_proj_bin.shape[0], 10]\n",
        "  except:\n",
        "    print('Exception Raised --> Linear Discriminant Analysis Config (classes exceeded)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuzd5QUtD88d"
      },
      "source": [
        "#APPLY FEATURE EXTRACTION PIPELINE ON PROJECTED FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCqXbF11D88e"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "#apply dimensionality reduction methods on principal components\n",
        "print('\\n--- --- --- --- ---\\nDimensionality Reduction Features Extraction: ')\n",
        "\n",
        "start_time = time.time()\n",
        "froid_light(data2)\n",
        "end = time.time()\n",
        "\n",
        "#save first feature extraction phase time ellapsed\n",
        "print(\"Support Features Extraction Time: %.8s seconds\" % (end - start_time))\n",
        "print('\\nNull Values in Dimensionality Reduction DataSets:')\n",
        "df_dataredu.dropna(axis=1, how=\"any\", thresh=None, subset=None, inplace=True)\n",
        "print(df_dataredu.isnull().sum())\n",
        "print('--- --- ---')\n",
        "\n",
        "#normalize calculated principal components\n",
        "scaler = RobustScaler()\n",
        "df_dataredu = pd.DataFrame(scaler.fit_transform(df_dataredu), columns=df_dataredu.columns)\n",
        "\n",
        "#save dimensionality reduction (applied on principal components) to .csv\n",
        "df_dataredu.to_csv(\"inception_dim_redu_original_data_light.csv\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "c-1PW4MHDATY",
        "s-5r-qRIDATY",
        "_bOjTQ72DATZ",
        "W0h5MIoF35Zs"
      ],
      "name": "FROID_LITE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}